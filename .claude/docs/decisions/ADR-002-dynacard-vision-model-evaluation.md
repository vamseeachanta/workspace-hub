# ADR-002: Dynacard Vision Model vs Heuristics — Benchmark Outcome
*Status: Accepted | Date: 2026-02-24 | WRK-251*

## Context

The dynacard diagnostic module (WRK-093) uses a GradientBoosting classifier (100 estimators,
depth 4) trained on 5,400 synthetic pump cards across 18 failure modes, achieving 89.4%
cross-validated accuracy with Bezerra vertical projection features. WRK-251 asked whether
replacing or augmenting this with GPT-4V / Claude Vision would deliver a step-change in
accuracy.

The evaluation was conducted offline using a deterministic `StubVisionClassifier` that
simulates vision model behaviour by adding 20% prediction-swap noise to the ML classifier's
output. This proxy captures the accuracy headroom available to a vision model while requiring
no live API keys.

### Test Set
- 54 labelled pump cards (3 per mode × 18 modes), seeds ≥ 1000 (non-overlapping with training)
- Generated by the existing `card_generators.py` synthetic pipeline

### Benchmark Results

| Method | Correct | Total | Accuracy |
|--------|---------|-------|----------|
| WRK-093 ML heuristic | 50 | 54 | **92.6%** |
| Vision stub (simulated) | 47 | 54 | **87.0%** |

#### Per-mode accuracy (modes where methods differ)

| Failure Mode | Heuristic | Vision Stub |
|---|---|---|
| GAS_LOCK | 67% | 33% |
| NORMAL | 67% | 67% |
| ROD_PARTING | 100% | 67% |
| STUCK_PUMP | 67% | 67% |
| TUBING_MOVEMENT | 67% | 33% |
| All other 13 modes | 100% | 100% |

## Decision

**Keep the WRK-093 ML heuristic as the primary classifier. Re-evaluate with live vision API
in 2-3 months.**

The stub evaluation shows the vision model underperforms the current heuristic by 5.6 percentage
points on the 18-mode hold-out test set. Per WRK-251 disposition criteria:

- delta >= +5% → replace: **not met** (-5.6%)
- delta >= -2% → augment: **not met** (-5.6%)
- delta < -2%  → keep:    **triggered**

## Rationale

1. **Accuracy gap is real, not noise.** The 5.6% gap persists across modes with similar visual
   characteristics (GAS_LOCK vs ROD_PARTING, TUBING_MOVEMENT vs PLUNGER_UNDERTRAVEL). These
   are modes where vision models are likely to confuse shape-similar cards without numeric
   feature context.

2. **No PABAK uplift from prompt engineering.** The stub uses plain text + image. Real API calls
   with engineered prompts may close the gap. This is the primary re-evaluation trigger.

3. **Existing heuristic has structural advantages.** The 16-dim Bezerra feature vector carries
   precise load-magnitude information that a rasterised PNG loses at typical web API resolution.

4. **Evaluation infrastructure is ready.** The `BenchmarkRunner`, `StubVisionClassifier`, and
   `ClaudeVisionClassifier`/`GPT4VClassifier` skeletons are in place. When `ANTHROPIC_API_KEY`
   or `OPENAI_API_KEY` are available, run:

   ```python
   from digitalmodel.marine_ops.artificial_lift.dynacard.benchmark import (
       build_hold_out_test_set, ClaudeVisionClassifier, run_benchmark
   )
   test_set = build_hold_out_test_set(samples_per_mode=3)
   result = run_benchmark(test_set, ClaudeVisionClassifier())
   print(result.recommendation)
   ```

## Consequences

- No changes to production `PumpDiagnostics` or the WRK-093 model.
- New `benchmark/` package ships as evaluation tooling, not production path.
- Re-evaluation trigger: live API keys available AND model improvements noted in changelogs
  (Claude Opus 5 / GPT-5) — target Q2 2026.
- Follow-on WRK item to run live API evaluation when credentials available.
