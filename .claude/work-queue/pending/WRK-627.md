---
id: WRK-627
title: "feat(frontierdeepwater): engineering AI demo — workflows + BSEE data + demo package (Days 4-5)"
status: pending
priority: high
complexity: complex
compound: false
created_at: 2026-02-26T00:00:00Z
target_repos:
  - frontierdeepwater
commit:
spec_ref:
parent: WRK-628
related:
  - WRK-625
  - WRK-626
  - WRK-391
blocked_by:
  - WRK-626
synced_to: []
plan_reviewed: false
plan_approved: false
percent_complete: 0
brochure_status: n/a
computer: ace-linux-1
execution_workstations: [ace-linux-1]
plan_workstations: [ace-linux-1]
provider: claude
orchestrator: claude
---

# feat(frontierdeepwater): Engineering AI Demo — Workflows + BSEE Data + Demo Package (Days 4-5)

## What

Build the Day 4–5 multi-step workflows, external data integration, and polished demo package.

### Day 4 — Multi-Step Analysis Workflows

**Rigid jumper design screening workflow:**

File: `ai-initiatives/demo/workflow_rigid_jumper_screening.md`

Prompt chain:
1. Input: water depth (ft), pipe OD (in), SMYS (ksi), design pressure (psi), temperature (°F)
2. Step 1 → wall thickness sizing (ASME B31.8)
3. Step 2 → combined stress check (Von Mises vs η×SMYS)
4. Step 3 → preliminary fatigue screening (V_r check, DFF flag)
5. Output: Go/No-Go table with governing criteria and flags for detailed analysis

**Pipeline route assessment workflow:**

File: `ai-initiatives/demo/workflow_pipeline_route_assessment.md`

Prompt chain:
1. Input: route length, water depth range, soil type, product (gas/liquid), OD, MAOP
2. Step 1 → wall thickness (DNV-OS-F101 pressure + collapse + prop buckling)
3. Step 2 → on-bottom stability screening (DNV-RP-F109)
4. Step 3 → free span allowable length check (DNV-RP-F105 simplified)
5. Step 4 → installation feasibility flag (S-lay/J-lay/reel based on WD + OD)
6. Output: summary table with flags for items requiring detailed analysis

**BSEE and GoA data integration:**

File: `ai-initiatives/demo/bsee_data_query_prompts.md`

- GOM deepwater pipeline/riser installation statistics (public BSEE data center)
- Gulf of Alaska offshore activity: environmental data, regulatory requirements, extreme metocean
- GoA-specific considerations: seismic (API RP 1111), ice loading, environmental sensitivity
- GoM vs GoA comparison prompt template: wave/current/wind/seismic parameters, code differences
- Project benchmarking prompt: compare design parameters against industry database

### Day 5 — Demo Package + Presentation

**Demo script:** `ai-initiatives/demo/demo_script.md`

15–20 min flow per discipline team:
- 2 min: intro and context ("not replacing engineers — augmenting")
- 5 min: quick Q&A (code lookups, material data, SCF tables)
- 5 min: live calculation (wall thickness or stress check with step-by-step)
- 3 min: data processing (simulation output → summary table)
- 3 min: document generation (calc memo or SOW)
- 2 min: Q&A and next steps

**Sample questions file:** `ai-initiatives/demo/sample_questions.md`
- 8 questions per discipline (mix of lookups, calculations, FFS, data)
- Curated from the proposal Q&A table
- Include expected output format for each

**FAQ and objections:** `ai-initiatives/demo/faq_objections.md`
- "What about accuracy?" / "What about IP security?" / "I'm faster manually"
- Talking points from proposal (30–50% time reduction data)

**Success criteria checklist:** from the roadmap:
- Both teams see 3+ features they'd use regularly
- At least 1 "I didn't know it could do that" moment
- Agreement on a 2-week pilot with 1–2 engineers per team
- 5 real project questions identified for pilot

## Why

Multi-step workflows are the most impressive demo feature — they show AI performing
engineering judgment (flagging items for detailed analysis) not just computation.
The BSEE/GoA data component differentiates the demo by showing real data queries,
especially the GoA angle which is non-standard for many engineers.
The demo script ensures the 30-minute window is tight and credibility-building.

## Acceptance Criteria

- [ ] Rigid jumper screening workflow prompt chain documented with sample I/O
- [ ] Pipeline route assessment workflow prompt chain documented with sample I/O
- [ ] BSEE/GoA data query prompts created and tested against public data
- [ ] Demo script created (15-20 min flow, both disciplines)
- [ ] Sample questions file: 8 per discipline with expected output format
- [ ] FAQ/objections doc created
- [ ] Full end-to-end dry run completed for both disciplines (documented in pilot/)
- [ ] No client-specific proprietary data in any file
- [ ] Legal scan passes

## Agentic AI Horizon

Complex because workflows require engineering judgment to validate multi-step logic.
Claude is primary (orchestration + engineering validation); Codex drafts the files.
BSEE data queries require live web access — use Claude with web search capability.
Recommend this runs on ace-linux-1 with Claude as orchestrator and Codex for file drafting.

## Path to Phase 2 (Post-Demo)

Once demo succeeds:
- Week 2-3: pilot with 2-3 engineers per team using real project questions
- Week 4: evaluate ROI, decide on browser-based chatbot deployment
- Capture in new WRK items when demo feedback is in
