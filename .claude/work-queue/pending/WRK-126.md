---
id: WRK-126
title: "Benchmark all example models across time domain and frequency domain with seed equivalence"
status: pending
priority: high
complexity: complex
compound: false
created_at: 2026-02-11T18:00:00Z
target_repos:
  - digitalmodel
commit:
spec_ref:
related: [WRK-099, WRK-031, WRK-121, WRK-125]
blocked_by: []
synced_to: []
plan: inline
plan_reviewed: true
plan_approved: true
percent_complete: 0
brochure_status:
provider: claude
provider_alt:
task_agents:
  phase_1: gemini   # Model inventory & classification — research
  phase_2: codex    # Statics baseline — run benchmarks
  phase_3: codex    # Time domain benchmark
  phase_4: codex    # Frequency domain benchmark
  phase_5: codex    # Seed equivalence analysis
  phase_6: gemini   # Consolidated report — summarization
tags: [benchmark, time-domain, frequency-domain, seed-equivalence, examples, validation]
---

# Benchmark All Example Models — Time Domain & Frequency Domain with Seed Equivalence

## What
Establish comparison benchmarks for all example models across both time domain and frequency domain analysis. Run each model with different random seeds to demonstrate analysis equivalence and statistical consistency. Cover all existing examples in the digitalmodel repository.

## Why
- Validates that example models produce consistent results across analysis domains
- Seed variation confirms results are not artifacts of a single random realization
- Provides a comprehensive regression/validation baseline for all examples
- Builds confidence in the analysis framework for both frequency-domain (RAOs, added mass, damping) and time-domain (motion histories, tensions, statistics)

## Scope
1. **Inventory all example models** in digitalmodel (frequency domain and time domain)
2. **Frequency domain benchmarks**: Run each applicable example, compare RAOs, added mass, damping, wave forces
3. **Time domain benchmarks**: Run each applicable example with multiple seeds, compare statistics (mean, std, max, min)
4. **Seed equivalence**: For each time domain example, run N seeds (e.g., 5-10), demonstrate statistical convergence
5. **Comparison metrics**: Correlation, relative error, statistical tests for seed-to-seed equivalence
6. **Results summary**: Per-example benchmark report with pass/fail criteria

## Acceptance Criteria
- [ ] All example models inventoried and categorized (frequency domain vs time domain vs both)
- [ ] Frequency domain benchmarks run and results compared for all applicable examples
- [ ] Time domain benchmarks run with multiple seeds for all applicable examples
- [ ] Seed equivalence demonstrated (statistics converge across seeds)
- [ ] Benchmark results documented with comparison metrics
- [ ] Pass/fail criteria defined and applied to each example
- [ ] Regression baseline established for future comparisons

## Plan

**Target**: `scripts/benchmark_model_library.py` (extend) + `scripts/seed_equivalence.py` (new)

### Existing Assets (reuse)
- `scripts/benchmark_model_library.py` — 3-way benchmark, 51 models validated (Path A/B/C)
- `scripts/benchmark_riser_library.py` — riser-specific benchmarking
- `scripts/semantic_validate.py` — section-by-section comparison pre-gate
- `benchmark_output/model_library_benchmark.json` — existing baseline (7.4 MB)
- New s7 models from WRK-121: ~14 spec.yml groups (jumper, installation, mooring, training)

### Phase 1 — Model Inventory & Classification
- Consolidate all example models: 51 existing library + ~14 new s7 groups + 4 riser models
- Classify each as: statics-only, time-domain, frequency-domain, or both
- Tag frequency-domain applicability: models with wave response, vessel RAOs, diffraction
- Output: `benchmark_output/model_inventory.yaml` with classification per model

### Phase 2 — Statics Baseline (existing, extend)
- Run existing 3-way benchmark on full inventory (Path A → B → C)
- Include new s7 models (jumper + installation + mooring)
- Establish statics convergence as pass/fail gate for time/freq domain runs
- Tolerance: tension <1%, bending <5% (existing criteria)

### Phase 3 — Time Domain Benchmark
- For each time-domain model: run with N=5 seeds (random wave seeds)
- **Pre-check**: detect models with fixed/hardcoded seeds in input files — flag these as "seed-invariant" (they pass trivially but don't test statistical variation)
- Duration: full simulation per model spec (typically 3-hour storm + build-up)
- Extract statistics per seed: mean, std, max, min for key results (tension, bending, vessel motion)
- Store per-seed results in `benchmark_output/seed_equivalence/<model>/`

### Phase 4 — Frequency Domain Benchmark
- For models with vessel RAOs/diffraction: compare frequency-domain transfer functions
- Use existing `run_3way_benchmark.py` for hull models (barge, ship, spar)
- Compare: RAO amplitude/phase across headings, added mass, radiation damping
- Tolerance: RAO amplitude <5%, phase <10deg (existing from diffraction benchmarks)

### Phase 5 — Seed Equivalence Analysis
- Statistical convergence: plot mean/std vs N_seeds (1-5), show convergence
- Seed-to-seed CoV (coefficient of variation) for each response quantity
- Pass criterion: CoV < 10% for Hs-normalized statistics
- Confidence intervals: 95% CI on extreme statistics
- Output: `benchmark_output/seed_equivalence_report.html`

### Phase 6 — Consolidated Report
- Per-model summary: statics pass/fail, time-domain stats, freq-domain comparison
- Global dashboard: model x metric matrix (green/yellow/red)
- Regression baseline JSON for future comparison
- Duration estimate: ~8-12 hours for full run (51+ models × 5 seeds × full simulation duration for TD). Plan for overnight/batch execution.
- **Test tiers**: Unit tests (no solver, mock data) run always. Solver-integration tests (need OrcFxAPI) run on licensed machine. Full benchmark (nightly/manual).

---
*Source: benchmark or establish comparison for all example models for time domain and frequency domain analysis; establish equivalence of analysis by running different seeds. We will do this for all examples*

## Agentic AI Horizon

- Benchmark infrastructure across all digitalmodel examples with seed equivalence establishes the validation suite that gives confidence in parametric runs — essential quality infrastructure.
- Benchmark execution and result comparison is a perfect agent task once the framework exists; building the framework now creates compounding value.
- **Disposition: invest now** — validation benchmarks are foundational; every new analysis module needs to demonstrate seed equivalence; build this framework now so it applies to all future work.
