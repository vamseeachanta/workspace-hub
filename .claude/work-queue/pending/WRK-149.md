---
id: WRK-149
title: "digitalmodel test coverage improvement (re-creates WRK-051)"
status: pending
priority: high
complexity: complex
created_at: 2026-02-16T00:00:00Z
target_repos:
  - digitalmodel
target_module:
commit:
spec_ref:
related: [WRK-051]
blocked_by: []
plan: inline
plan_reviewed: true
plan_approved: false
percent_complete: 0
brochure_status: n/a
provider: claude
provider_alt:
---

# digitalmodel Test Coverage Improvement

## What
Increase digitalmodel test coverage from ~2.7% to at least 50% (stretch: 80%). WRK-051 was planned and cross-reviewed but never executed. This item resurrects it with a realistic phased approach.

## Why
- 83K lines of production code at 2.7% coverage is the highest technical risk in the workspace
- Active development (wall thickness, hydrodynamics, hull library, FFS) makes regressions likely
- Blocks confidence in refactoring and new feature development

## Acceptance Criteria
- [ ] Test collection fixed (pytest discovers all test files)
- [ ] Baseline coverage measured and documented
- [ ] Priority modules covered first: structural (wall_thickness), hydrodynamics (diffraction), hull_library
- [ ] At least 200 new tests written across priority modules
- [ ] Coverage reaches 50%+ on priority modules
- [ ] CI-compatible test runner configured

## Notes
- Original WRK-051 plan (archived, cross-reviewed 2026-02-09) has detailed per-module breakdown — reference it
- Use existing 3-tier test runner from WRK-119 (test-commit.sh, test-task.sh, test-session.sh)
- Prioritize modules with active development over legacy/stable code

## Plan

**Target**: `tests/` across digitalmodel (extend existing)

### Existing Assets
| Component | Tests | Coverage |
|-----------|-------|----------|
| wall_thickness | 214 | Good |
| fatigue | 493 | Good |
| diffraction/benchmark | 25+ | Moderate |
| hull_library | 13 | Low |
| orcaflex/format_converter | 102 | Good |
| asset_integrity | ~4 effective | Critical gap |
| gis | 18 | Moderate |

### Phase 1 — Baseline Measurement
- Run full test collection: `python3 -m pytest tests/ --collect-only`
- Measure coverage: `python3 -m pytest tests/ --cov=src/digitalmodel --cov-report=html`
- Document: total tests, pass rate, coverage % per module
- Identify untested public APIs

### Phase 2 — Priority Module Tests (highest risk first)
- **asset_integrity** (~4 effective tests → 40+): fix shams, add grid parser, FFS assessment tests
- **hull_library** (13 → 40+): parametric hull generation, catalog validation, mesh quality
- **hydrodynamics** (25 → 60+): RAO extraction, solver comparison, hydrostatic validation
- **orcaflex core** (low → 30+): model interface, configuration, analysis engine

### Phase 3 — Cross-Cutting Tests
- Integration tests: module interactions (e.g., fatigue + orcaflex result extraction)
- Regression tests: pin current behavior for actively developed modules
- Edge cases: empty inputs, missing data, unit conversion boundaries

### Phase 4 — CI Integration
- Configure pytest markers for tiered execution (commit/task/session)
- Ensure no license-dependent tests run in CI (use `@pytest.mark.skipif`)
- Coverage gates: fail if coverage drops below baseline

### Test Strategy
- TDD for new test gaps
- Use existing 3-tier runner: test-commit.sh (fast), test-task.sh (moderate), test-session.sh (full)
- Priority: asset_integrity > hull_library > hydrodynamics > orcaflex core
- Target: 200+ new tests, 50%+ coverage on priority modules
- Run: `PYTHONPATH="src:../assetutilities/src" python3 -m pytest tests/ -v --tb=short --noconftest`
