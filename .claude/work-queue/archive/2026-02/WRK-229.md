---
id: WRK-229
title: Skills curation — online research, knowledge graph review, update index, session-input health check
status: archived
priority: high
complexity: medium
compound: false
created_at: 2026-02-20T00:00:00Z
completed_at: 2026-02-24T00:00:00Z
target_repos:
  - workspace-hub
commit:
spec_ref:
related: [WRK-225, WRK-226, WRK-228, WRK-230, WRK-231, WRK-234]
blocked_by: []
synced_to: []
plan_reviewed: false
plan_approved: true
standing: true
cadence: dynamic  # starts daily; auto-adjusts to weekly/monthly based on signal yield
auto_execute: true
percent_complete: 100
brochure_status: n/a
computer: ace-linux-1
---
# Skills curation — periodic online research, update index, and session-input health check

## What

Three standing mechanisms for keeping the skills library alive, well-targeted, and high-signal:

1. **Periodic online research** — at a dynamically adjusted cadence (daily → weekly → monthly), an agent researches the web for developments relevant to skills in the library, identifies gaps or stale content, and updates, curates, or creates new skills as warranted. New skill identification is first-class: if research or graph review surfaces a capability gap with no existing skill, a new skill is created, not just an update logged. The cadence shortens when research yields strong signals, lengthens when yield is low.

2. **Knowledge graph review** — at the same cadence as online research, analyse the skills knowledge graph (`SKILLS_GRAPH.yaml`) against active work items and repo activity to identify: which skill domains are heavily used but data-thin, which WRK items cluster around under-served skills, and which skills have no connections to current work (candidates for archival). Outputs a prioritised list of skills that need more data — this list directly feeds the online research agent's search targets for that run.

3. **Session-input health check** — verify that sessions are actually contributing to skills. If the `skill-learner` post-commit hook and session capture pipeline are running, they should be feeding new patterns into `.claude/skills/` and `.claude/state/`. This health check confirms the pipeline is live and producing output, and flags when it has gone quiet.

## Why

A skills library that isn't growing from external research and live session work is stale by default. Worse, without a knowledge graph review, research effort is blind — it doesn't know which skill domains are actually load-bearing for current repo work and which are orphaned. The knowledge graph closes this loop: it identifies where skill data is thin relative to actual demand, so research effort is directed, not scattered. The ecosystem already has the `skill-learner` post-commit hook and session memory hooks — but there is no mechanism to confirm they are producing output, no agent actively researching online, and no graph-driven prioritisation of where to focus. All three gaps are addressed here.

## Acceptance Criteria

### Online Research Pipeline
- [ ] Create a `skills-researcher` skill at `.claude/skills/coordination/workspace/skills-researcher/SKILL.md`
- [ ] Skill runs on a dynamic cadence: starts daily, auto-adjusts based on yield (high yield = maintain frequency; low yield = step down to weekly, then monthly)
- [ ] Each run: searches for developments relevant to top N skills by usage/recency; identifies new tools, patterns, or deprecations; updates or curates existing skills; **identifies and creates new skills** where a capability gap exists with no current coverage; logs yield (# updates + # new skills created per run)
- [ ] Cadence adjustment logic: if 3 consecutive runs yield < threshold findings → step down frequency; if yield spikes → step back up
- [ ] Maintains a research log at `.claude/state/skills-research-log.jsonl`
- [ ] Updates `skill-registry.yaml` and skill INDEX files after each curation pass

### Knowledge Graph Review
- [ ] At each curation run: load `SKILLS_GRAPH.yaml` + active WRK items (pending/working/blocked) + recent git commit history
- [ ] Score each skill domain by: (a) frequency referenced in active WRK items, (b) commit activity in related repos, (c) current skill depth (# connected skills, content length, last updated)
- [ ] Output a **priority list**: skill domains with high demand but low data — top N become the research agent's search targets for that run
- [ ] Flag skills with zero connections to current work as archival candidates
- [ ] Flag capability gaps (high-demand domain, no skill exists) as **new skill candidates** — passed to the research agent as creation targets for that run
- [ ] For gaps that are deep or require domain expertise to fill properly — i.e. a quick research pass won't suffice — **automatically spin off a new WRK item** so the user can engage: describe the gap, why it matters, what data/research is needed, and a suggested approach. These are not auto-resolved; they surface for user collaboration.
- [ ] Log graph analysis results to `.claude/state/skills-graph-review-log.jsonl`

**Gap triage logic:**
- Shallow gap (known domain, missing coverage) → agent creates skill autonomously
- Deep gap (requires domain expertise, significant research, or user input) → spin off `WRK-NNN: [domain] skill gap — needs user input` with context, demand evidence, and suggested approach

### Session-Input Health Check
- [ ] Add a health check to the weekly session review (or `ensure-readiness.sh`) that verifies:
  - `skill-learner` hook has fired at least once in the last N sessions
  - `.claude/state/sessions/` contains skill-related entries (pattern captures, skill candidates)
  - New skills or skill updates have been committed in the last 7 days
- [ ] Health check emits a warning if the pipeline has gone quiet, with a diagnostic message identifying which stage is silent
- [ ] Integrate with WRK-226 preflight hook work — session health and skills health are related signals

## Holistic Note — Session Lifecycle Overlap

The gap surfacing concept here runs at a periodic cadence (daily/weekly). The same concept should also fire at **end of session** — the session just ran, skills were exercised, knowledge limits were hit, and the context is richest right now. `hooks/session-review.sh` already extracts skill candidates and patterns at session end but is not yet explicitly pointed at gap identification or WRK item creation.

Related items that touch the same session lifecycle junction:
- `hooks/session-review.sh` — existing end-of-session hook (extend, don't duplicate)
- WRK-177: Stop Hook — Engineering Calculation Audit Trail
- WRK-178: Stop Hook — Data Provenance Snapshot
- WRK-226: Agent performance audit (session noise analysis)
- **WRK-230**: Holistic session lifecycle review — unifies gap surfacing, stop hooks, and skill input pipeline across session start/end and periodic cadences *(new item, see below)*

**WRK-229 owns the periodic cadence. WRK-230 owns the session-end trigger.** Both feed the same output: new skills, updated skills, and deep-gap WRK items for user collaboration.

## Agentic AI Horizon

- This work *is* future-boosting infrastructure — a self-improving skills library is a direct multiplier on agent capability
- The online research pipeline is exactly the kind of standing autonomous work that agentic AI should own — no human should be manually curating skills
- In 3–4 months, research agents will be significantly better at this; build the scaffolding now so the better agents slot straight in
- **Disposition**: do now — lay the pipeline infrastructure; expect the research quality to improve as models improve without rearchitecting

---
*Source: "for skills update/curation, we should also add periodic (daily, weekly, monthly — frequency dynamically adjusted based on outcome) to research online and maintain an update index. also health check that our sessions are providing some inputs to these skills" + "knowledge graph to identify high priority items or skills that need more data for the work we are doing in the repo ecosystem — added to same workflow junction as WRK-229"*
