---
id: WRK-231
title: "session-analysis skill — first-class session mining as foundation for skills, gaps, and agent improvement"
status: done
priority: high
complexity: medium
compound: false
created_at: 2026-02-20T00:00:00Z
target_repos:
  - workspace-hub
commit:
spec_ref:
related: [WRK-226, WRK-228, WRK-229, WRK-230, WRK-232, WRK-234, WRK-236, WRK-237]
blocked_by: []
synced_to: []
plan_reviewed: false
plan_approved: true
standing: true
auto_execute: true
cadence: per-session  # fires at every session end
percent_complete: 100
completed_at: 2026-02-20T00:00:00Z
brochure_status: n/a
---

# session-analysis skill — first-class session mining as foundation for skills, gaps, and agent improvement

## The Self-Learning Loop (linear)

Despite the complexity of what's tracked internally, the user-facing flow must be simple and linear. Knowledge graphs hold the complexity — the workflow is a straight line:

```
SESSION RUNS
     ↓
SESSION ENDS → session-analysis fires automatically
     ↓
ANALYSIS PRODUCES:
  • Skill scores (used? one-shot? corrections?)
  • Tool anti-patterns
  • Script candidates (→ agentic uplift)
  • Knowledge gaps (shallow / deep)
  • User intervention events
     ↓
ROUTE EACH SIGNAL:
  Shallow gap       → skill created/updated (WRK-229)
  Deep gap          → WRK item created (user reviews)
  Script candidate  → script-candidates.md
  Skill candidate   → skill-candidates.md
  Hook candidate    → hook-candidates.md
  Agent candidate   → agent-candidates.md
  MCP candidate     → mcp-candidates.md
  Anti-pattern      → preflight hook updated (WRK-226)
  Skill score low   → curation priority raised (WRK-229)
     ↓
NEXT SESSION STARTS with:
  Better skills · Less noise · Flagged gaps already in queue
     ↓
REPEAT → system improves with every session
```

The complexity (knowledge graphs, scorecards, indexes) is invisible to the user. What the user sees: gaps surface as WRK items, sessions get smoother over time.

## What

Create a `session-analysis` skill that wraps, improves, and unifies the existing session-end hooks into a first-class, versioned, self-improving capability. This is the holy grail: deep analysis of every session surfaces knowledge gaps, skill improvement targets, and agent behaviour patterns — and everything else (skills curation, noise reduction, gap surfacing, lifecycle health) falls out of it naturally.

## Why

Sessions are where valuable time is spent. Every session is a signal-rich event: the agent attempted things, succeeded or failed, hit knowledge limits, used skills well or poorly, and received corrections. That signal is currently almost entirely wasted — hooks fire quietly, log shallow data, and nothing feeds back into improving the system. The hooks exist (`session-review.sh`, `session-end-evaluate.sh`, `generate-session-report.sh`) but they do shallow analysis: keyword matching, tool counts, pattern detection. There is no skill wrapping them, no improvement tracking, no semantic gap identification, and no mechanism to turn what the agent *didn't know* into actionable output. A first-class skill:
- Is versioned and can be improved incrementally
- Has explicit quality metrics (is the analysis getting better?)
- Does semantic analysis, not just keyword matching
- Produces structured, actionable output fed directly into WRK-229 (curation), WRK-230 (lifecycle), and WRK-226 (noise audit)
- Creates WRK items automatically for deep knowledge gaps

## Architecture Decision — Two-Stage Design

**Session hooks must stay simple.** They run at a sensitive moment (session close), must be fast, and silent failures cause lost data. Keep them to raw signal capture only.

**Heavy analysis runs on morning cron at 3 AM** — 2 hours before `claude-reflect` (5 AM), giving enough time for the analysis to complete before the workday starts. Reads previous day's signals, does deep analysis, updates scorecards, creates WRK items.

```
SESSION END (hook — lightweight, <2s)
  └── Capture raw signals to state/sessions/
        • Skill invocations (name, outcome)
        • Tool calls (sequence, anti-patterns)
        • Script calls (name, success/fail)
        • User correction events (classified)
        • New files/scripts written
      → Write to: state/session-signals/YYYY-MM-DD-HHMMSS.jsonl
      → Nothing else. No analysis. No WRK creation.

MORNING CRON 3AM (heavy analysis — no time pressure)
  └── Read all signal files from previous day
        • Score skills against scorecard
        • Detect anti-patterns and trends
        • Identify knowledge gaps
        • Generate script candidates
        • Update tool knowledge graph
        • Update skill-registry.yaml scores
      → Route outputs:
          Shallow gaps      → skills curation (WRK-229)
          Deep gaps         → new WRK items
          Anti-patterns     → preflight hook update (WRK-226)
          Script candidates → script-candidates.md (bash sequences → scripts)
          Skill candidates  → skill-candidates.md (patterns worth wrapping as skills)
          Hook candidates   → hook-candidates.md (scripts → hooks, recurring triggers)
          Agent candidates  → agent-candidates.md (tasks → reusable agent definitions)
          MCP candidates    → mcp-candidates.md (tools worth adding as MCP servers)
```

**Cross-machine regrouping:** `ace-linux-1` (this machine) is the primary. On the morning cron it checks whether signal files have arrived from other machines (via git pull or shared mount). If updates arrived → aggregate with local signals and analyse. If nothing arrived → analyse local sessions only and proceed. No push required from other machines — they write locally, this machine pulls when available. Simple pull-based model, no distributed infrastructure.

## Current State vs Target

| Aspect | Current | Target |
|--------|---------|--------|
| Trigger | Stop hook (fires but not monitored) | Dedicated skill, stop hook as trigger, monitored for output |
| Analysis depth | Keyword match, tool counts | Semantic: what was asked, what failed, what gaps were hit |
| Gap detection | None | Explicit: track low-confidence responses, repeated fallbacks, unanswered questions |
| Output | JSONL logs, pending-reviews/ | Structured gap report + WRK items for deep gaps + skills feed for WRK-229 |
| Improvement | None — hooks don't evolve | Tracked: analysis quality metrics per session, skill version history |
| Visibility | Silent — hooks fire quietly | Observable: session analysis summary surfaced at next session start |

## What to Analyse in a Session

1. **Knowledge gaps** — where did the agent hedge, fall back to training data, or produce low-confidence answers? What topics were repeatedly probed without resolution?
2. **Skill usage vs skill quality** — which skills were invoked? Did they produce good output or did the agent work around them?
3. **Tool patterns** — repeated sequences that should become skills; anti-patterns that indicate poor orchestration
4. **Corrections and overrides** — where did the user correct the agent? These are high-signal learning moments
5. **Context noise** — what was loaded at session start but never used? (feeds WRK-226)
6. **Work item signal** — which WRK items were touched? What progress was made? Any blockers surfaced?
7. **Context window spend** — which context sections consumed the most tokens; sections loaded but never referenced during the session (token-level companion to context noise, feeds WRK-226)
8. **Recurring error patterns** — same error appearing in 3+ sessions = systemic issue, not one-off; auto-surface as a WRK item candidate
9. **Session goal completion** — stated intent at session start vs work actually shipped; scope creep detection; low completion rate = items too large or context too noisy
10. **Work queue velocity** — items queued vs completed per session; if net queue growth persists across sessions, throughput model is broken

### Tool Use Assessment (per-tool, per-session)

Tools (Read, Write, Edit, Bash, Grep, Glob, Task, WebSearch, etc.) are the primitives beneath skills. Tracking tool use independently of skills surfaces anti-patterns, capability gaps, and inefficiency that skill-level analysis misses.

| Signal | What to capture |
|--------|----------------|
| **Frequency** | How often was each tool called in the session? |
| **Sequences** | Which tools appear in recurring sequences? (e.g. Read → Grep → Edit = pattern; cat via Bash instead of Read = anti-pattern) |
| **Redundancy** | Was the same tool called multiple times for the same target? Indicates iteration/failure |
| **Anti-patterns** | Bash used where a dedicated tool exists (grep instead of Grep, cat instead of Read) — these are rule violations |
| **Delegation rate** | Task tool usage as % of total — low rate on complex sessions = orchestrator pattern violation |
| **Outcome correlation** | Did high tool-call sessions have more user corrections? Indicates poor planning or wrong tool choices |

**Tool Use Index** — maintained at `.claude/state/tool-use-index.yaml`:
- Per-tool usage counts, trend over last 30 sessions
- Common co-occurrence pairs (tools used together)
- Top anti-patterns detected (with session evidence)
- Delegation score trend (Task tool %)

**Tool Knowledge Graph** — if tool co-occurrence patterns are complex enough, a dedicated graph at `.claude/state/tool-knowledge-graph.yaml` maps:
- Tool → typical context (what kinds of tasks invoke it)
- Tool → common successors (what follows it in typical sequences)
- Tool → skills that rely on it (connect tool graph to skill graph)
- Anomalies: tools appearing in contexts where they shouldn't

This graph feeds the skill modularity signal — if a skill always triggers the same 4-tool sequence, those tools should be encapsulated inside the skill rather than re-orchestrated every session.

### Skill Performance Tracking (per-skill, per-session)

For every skill that was active or referenced in a session, record:

| Signal | What to capture |
|--------|----------------|
| **Used?** | Was the skill actually invoked, or just loaded into context and ignored? |
| **One-shot?** | Did the skill produce the right output first time, or did the agent iterate, retry, or work around it? |
| **Corrections required** | How many user corrections followed the skill's output? Zero = excellent; 1–2 = improvable; 3+ = broken |
| **Improvement vector** | What specifically failed? Wrong scope, missing steps, outdated knowledge, poor structure, not modular enough? |
| **Knowledge graph consensus** | Does the skill's output align with connected skills in the graph? Contradictions or overlaps with related skills surface here |
| **Modularity signal** | Did the agent have to re-implement something the skill should have covered? Was the skill too broad (did too much, hard to reuse) or too narrow (needed alongside 3 others to do anything useful)? |

**Aggregated across sessions**, this produces a per-skill scorecard:
- Usage rate (invoked vs loaded-but-ignored)
- One-shot rate (%)
- Mean corrections per use
- Top improvement vectors (recurring failure modes)
- Graph consensus health (conflicts with related skills)
- Modularity score

Skills below threshold on any metric are flagged as improvement targets and fed to WRK-229 curation. Skills with zero usage over N sessions are flagged for archival review.

## Acceptance Criteria

- [ ] Create `.claude/skills/coordination/workspace/session-analysis/SKILL.md` — versioned, with explicit capabilities, two triggers (session-end hook + morning cron), and output spec
- [ ] Session-end hook: lightweight only — raw signal capture to `state/session-signals/`, <2s runtime, no analysis
- [ ] Morning cron (5 AM, same slot as claude-reflect): heavy analysis — reads previous day's signals, scores, routes outputs
- [ ] Cross-machine: `ace-linux-1` is primary aggregator — morning cron checks for signal files from other machines (git pull / shared mount); analyses what's available, local-only if nothing arrived
- [ ] Existing hooks (`session-review.sh`, `session-end-evaluate.sh`) simplified to signal capture only — heavy logic moved to cron job
- [ ] Analysis produces a structured session report: knowledge gaps (shallow/deep), skill quality signals, correction events, context noise candidates
- [ ] Per-tool usage record captured each session: frequency, sequences, redundancy, anti-patterns, delegation rate
- [ ] Tool use index maintained at `.claude/state/tool-use-index.yaml` — per-tool counts, co-occurrence pairs, anti-pattern log, delegation score trend
- [ ] Tool knowledge graph at `.claude/state/tool-knowledge-graph.yaml` — tool → context, tool → successors, tool → skills that rely on it; updated after each session
- [ ] Anti-patterns auto-flagged (e.g. Bash used for file ops, Grep skipped for Read+manual scan) — surfaced in session summary and fed to WRK-226 noise/preflight
- [ ] Per-skill performance record captured each session: used/ignored, one-shot rate, correction count, improvement vector, modularity signal
- [ ] Skill scorecard updated in `skill-registry.yaml` after each session: `usage_rate`, `one_shot_rate`, `mean_corrections`, `top_failure_modes`, `graph_consensus`, `modularity_score`
- [ ] Skills below threshold on any scorecard metric → flagged as improvement target → fed to WRK-229 as priority curation target
- [ ] Skills with zero usage over 10 sessions → flagged for archival review
- [ ] Knowledge graph consensus check: after each session, scan related skills for contradictions or overlaps introduced by recent skill updates
- [ ] Script usage tracked per session: scripts invoked, success/failure, new scripts written, reuse candidates
- [ ] `script-candidates.md` updated after each session: scripts → skills, bash sequences → scripts, scripts → hooks — the agentic uplift pipeline
- [ ] New capability detection: new libraries, APIs, MCP tools, file types surfaced as skill creation candidates
- [ ] Delegation quality scored per session: Task tool %, inline-vs-delegated ratio, subagent result quality
- [ ] Agent library health tracked: which agents spawned, output quality, over/under-delegation flagged, unused agents identified
- [ ] Hook health tracked: which hooks fired, output produced, silent failures flagged
- [ ] Git operations health: commit frequency, uncommitted-on-exit warning, submodule drift, branch age, commit message compliance
- [ ] Cross-machine conflict risk: `uncommitted_changes` flag in signal file; morning cron surfaces recommendation if pattern persists
- [ ] MCP health: calls made, failures, unused MCPs loaded into context
- [ ] Rules health: violations detected (anti-patterns), rules never triggered (noise candidates)
- [ ] WRK item velocity tracked: items advanced, stalled, friction points logged
- [ ] User intervention events classified: correction / redirect / clarification / override — rate tracked as primary agent quality metric
- [ ] Trending signals aggregated across sessions: skill improvement velocity, anti-pattern recurrence, WRK throughput, delegation score, gap closure rate, session length, new skill/script creation rate, git commit frequency trend
- [ ] Deep gaps → auto-create WRK item (`WRK-NNN: [topic] knowledge gap — session evidence: [session-id]`)
- [ ] Shallow gaps → feed directly to WRK-229 skills curation pipeline as research targets
- [ ] Context noise signals → feed to WRK-226 preflight hook
- [ ] Session analysis summary stored at `.claude/state/session-analysis/YYYY-MM-DD-HH.md` — readable by next session's preflight
- [ ] Context window spend tracked per session: which context sections used the most tokens, which sections were loaded but never referenced — fed to WRK-226
- [ ] Recurring error patterns detected across sessions: same error in 3+ sessions → auto-create WRK item candidate
- [ ] Session goal completion tracked: stated intent vs actual output per session; sessions with <50% completion flagged for review
- [ ] Work queue velocity tracked: items queued vs archived per session; net queue growth persisting >5 sessions → surfaced as systemic issue
- [ ] Quality metric: track % of sessions producing at least one actionable output (gap WRK, skill target, or noise signal) — target >80%
- [ ] Skill is improved after each 10-session batch based on quality metric — version bumped when analysis improves

### Additional Session Dimensions — Full Ecosystem Health

The session analysis covers everything we actively maintain in the repo that influences daily work — not just skills and tools. Every artefact that shapes how the agent operates is fair game for improvement signals.

**Full analysis scope:**

| Artefact | What to track |
|----------|--------------|
| **Skills** | Usage, one-shot rate, corrections, modularity (covered above) |
| **Tools** | Frequency, sequences, anti-patterns, delegation rate (covered above) |
| **Scripts** | Invocations, failures, reuse candidates, agentic uplift path |
| **Agents** (`.claude/agent-library/`) | Which agents were spawned? Did they produce good output? Were results used or discarded? Agent quality score per session |
| **Hooks** | Which hooks fired? Did they produce output? Silent failures? Hook health per session |
| **Git operations** | Commit frequency, branch state, submodule sync, cross-machine conflict risk (see below) |
| **MCP servers** | Which MCPs were called? Latency, failures, unused MCPs loaded into context |
| **Settings/config** | `settings.json`, `model-registry.yaml`, `behavior-contract.yaml` — were any settings ignored or contradicted during the session? |
| **Rules** | Which rules were violated (anti-patterns)? Which rules were never relevant (noise candidates)? |
| **Keybindings / UI config** | Were any friction points apparent in how commands were invoked? |

#### Git Operations Health

Git is daily infrastructure. Sessions reveal friction:

| Signal | What to capture |
|--------|----------------|
| **Commit frequency** | Sessions with no commits = work at risk; flag if >30 min of changes uncommitted |
| **Cross-machine conflict risk** | Uncommitted changes on session end = potential clash when another machine pulls; emit warning + suggest commit |
| **Submodule drift** | Submodule pointer not updated after inner commit = known friction source |
| **Branch age** | Working branches >3 days old without merge = stale work risk |
| **Force-push events** | Log when detected — high-risk operation |
| **Commit message quality** | Conventional Commits compliance; missing WRK reference |

**Cross-machine strategy**: at session end, if there are uncommitted changes, the signal file should flag `uncommitted_changes: true`. The morning cron on `ace-linux-1` checks this — if another machine has uncommitted changes across sessions, surface a recommendation to commit more frequently. The principle: **commit at every logical checkpoint, not just at task completion**.

#### Agent Library Health

`.claude/agent-library/` has 45 agent definitions. Sessions reveal which ones actually work:

- Which agents were spawned via Task tool?
- Did the agent complete its task or time out / produce poor output?
- Were agents spawned for tasks that a simple tool call could have handled? (over-delegation)
- Were complex tasks handled inline when they should have been delegated? (under-delegation)
- Agent definitions not used in last 30 sessions → archival candidates

#### Script Usage & Agentic Script Identification

| Signal | What to capture |
|--------|----------------|
| **Scripts invoked** | Which `.sh` / `.py` scripts were called via Bash? Frequency, success/failure, repeated calls |
| **New scripts written** | Did the agent write a new script in this session? File path, purpose, whether it was reused later in same session |
| **Script reuse candidates** | Scripts called 2+ times across sessions → candidate to wrap as a skill or promote to `scripts/` |
| **Agentic uplift candidates** | Manual bash sequences that could become scripts; scripts that could become hooks; hooks that could become skills — each is a step toward more autonomous operation |
| **Script quality signals** | Were scripts written with `#!/usr/bin/env bash`, LF endings, error handling? Anti-patterns flagged |

**Output**: a `script-candidates.md` list — scripts that should be promoted, hooks that should be created, skills that should wrap existing scripts. This is the direct path to making repos more agentic.

#### New Capability Detection

When the agent does something novel in a session — a new library used, a new API called, a new pattern applied — that's a signal for skill creation even if no gap was hit:

- New `import` / `require` statements in code written → library not yet in skills?
- New MCP tool calls → MCP server skill coverage adequate?
- New external services or APIs touched → should become a skill with auth patterns
- New file types created → any pattern worth capturing?

#### Agentic Delegation Quality

Sessions reveal how well the orchestrator pattern is being followed:

| Signal | Target |
|--------|--------|
| Task tool % of total calls | >30% on complex sessions |
| Inline execution of complex tasks (should have been delegated) | → flag as orchestration anti-pattern |
| Subagent result quality (did the Task agent produce good output?) | Feed into agent-library improvement |
| User had to re-do what an agent did | High-signal failure — log specifically |

#### WRK Item Velocity & Friction

- How many WRK items were advanced per session?
- Which items stalled (touched but not progressed)?
- Where did friction occur — plan gate, cross-review, blocked dependency?
- Items that took 3+ sessions to advance → candidate for decomposition

#### User Intervention Patterns

Every user correction, redirect, or clarification is a signal:

- **Correction** (agent was wrong) → skill or rule gap
- **Redirect** (agent went in wrong direction) → plan quality issue or ambiguous WRK item
- **Clarification** (agent asked unnecessarily) → missing context in CLAUDE.md or MEMORY.md
- **Override** (user rejected agent's autonomous decision) → autonomy boundary miscalibrated

Intervention rate per session is a direct measure of agent quality. Target: decreasing over time.

#### Communication Efficiency

- Exchanges per task (high = poor initial understanding or ambiguous instructions)
- Questions asked by agent (necessary vs unnecessary)
- User message length trend (long messages = agent not anticipating needs)

#### Repo Change Patterns

- Which repos were touched?
- Change type distribution: tests / features / config / docs / scripts / skills
- Test pass rate on first run vs after correction
- Files created that don't follow naming conventions → flag immediately

#### Trending Signals (across sessions, not just per-session)

Aggregated trends that only emerge across multiple sessions:

| Trend | Value |
|-------|-------|
| Skill improvement velocity | Are skill one-shot rates increasing over time? |
| Anti-pattern recurrence | Same anti-patterns repeating = systemic, not one-off |
| WRK item throughput | Items archived per week — is the queue shrinking or growing? |
| Delegation score trend | Is orchestration getting more autonomous over time? |
| Knowledge gap closure rate | Are gap WRK items being resolved, or accumulating? |
| Session length trend | Shorter sessions with same output = improving efficiency |
| New script/skill creation rate | Is the ecosystem growing its own capabilities? |

## Bootstrap — We Have the Data Now (see WRK-232)

**241 sessions already exist** in `.claude/state/sessions/` — 214 JSON + 27 JSONL files, 12MB total, spanning at least the last month. This is enough to bootstrap the skill scorecard immediately rather than waiting for future sessions. The first run of this skill should:

1. Mine the 27 JSONL session files (structured, most analysable) for skill invocations, correction events, and tool patterns
2. Produce an initial skill scorecard from real historical data
3. Identify the top 5 underperforming skills by correction rate and usage-vs-load ratio
4. Surface 2–3 deep knowledge gap WRK items from patterns in those sessions

This is not a greenfield build — we can validate the analysis quality against real data from day one.

## Agentic AI Horizon

- Session analysis is the foundational loop: better analysis → better skills → better agent → better sessions → better analysis
- This is the piece that makes everything else self-improving rather than manually maintained
- In 3–4 months models will be significantly better at semantic gap detection — the skill scaffolding built now will benefit immediately as models improve
- **Disposition**: do now, invest heavily — this is the highest-leverage item in the cluster. Everything else depends on the quality of session analysis.

---
*Source: "how are we analyzing the session — that is where the holy grail starts. do we have a skill for it. is it being improved. if so rest will fall in place"*
