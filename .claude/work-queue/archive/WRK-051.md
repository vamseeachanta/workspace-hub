---
id: WRK-051
title: "digitalmodel test coverage improvement"
status: archived
priority: high
complexity: complex
created_at: 2026-01-30T00:00:00Z
target_repos:
  - digitalmodel
commit:
spec_ref:
related: [WRK-052, WRK-053, WRK-054, WRK-055, WRK-056]
blocked_by: []
merged_from: [WRK-087]
synced_to: []
tags: [testing, coverage, tdd]
---
# digitalmodel Test Coverage Improvement

## What
Improve test coverage for the digitalmodel repository from the current 2.7% to 80%+. The repo has 83K lines and 4064 test files that exist but fail to cover the source code properly.

## Why
At 2.7% coverage, the digitalmodel codebase has virtually no safety net for regressions. The test files exist but do not properly import or exercise the digitalmodel package, meaning refactoring and new features carry high risk.

## Current State
- **Coverage**: 2.7%
- **Source size**: ~83K lines
- **Test files**: 4064 (exist but don't cover source)
- **Root cause**: Test collection does not import the digitalmodel package correctly

## Strategy
1. Fix test collection to properly import the digitalmodel package
2. Prioritize coverage for critical packages: orcaflex, fatigue, marine, signal
3. Add integration tests for core workflows
4. Ensure pytest discovers and runs all existing test files against source

## Acceptance Criteria
- [ ] Test collection fixed to import digitalmodel package
- [ ] Coverage for orcaflex package reaches 80%+
- [ ] Coverage for fatigue package reaches 80%+
- [ ] Coverage for marine package reaches 80%+
- [ ] Coverage for signal package reaches 80%+
- [ ] Overall repository coverage reaches 80%+
- [ ] CI pipeline reports coverage on every PR

## Notes
- Large codebase -- prioritize packages by business criticality
- Existing test files may need path/import fixes rather than rewriting from scratch

## Plan

**Spec**: `specs/modules/wrk-051-digitalmodel-test-coverage.md`
**Target**: `digitalmodel/tests/`

### Current State (from research)
- **1,097 source files**, ~488 test files (verified on disk), 2.7% coverage
- **Critical zero-coverage gaps**: visualization/ (0%), data_systems/ (1.8%), common/ (low)
- **Working test patterns**: marine_ops/ (dynacard has 83 tests), orcaflex/ (converter tests exist)
- **Root cause**: Many test files exist but fail to import `digitalmodel` package correctly (PYTHONPATH issues)
- **Test runner**: `PYTHONPATH="src:../assetutilities/src" python3 -m pytest --noconftest`

### Phase 1 — Fix Test Collection Infrastructure
- Investigate whether `digitalmodel` is properly installable as a package (`pip install -e .` or `uv pip install -e .`). If not, fix packaging before conftest.
- Fix `conftest.py` to properly set up `digitalmodel` package imports
- Audit test discovery: which of ~488 test files actually run vs. skip/error
- Create `pytest.ini` or `pyproject.toml` [tool.pytest] section with correct paths
- Target: all existing test files should at minimum be discovered by pytest

### Phase 2 — Priority Tiers (by business criticality)
**Tier 1 — Critical (0% → 80%)**:
- `visualization/` — SVG generation, Plotly charts, schematics (0 tests, 7+ modules)
- `data_systems/` — data loaders, schemas, catalog (1.8%, 2 tests)
- `common/` — shared utilities used across all modules

**Tier 2 — High**:
- `marine_ops/` — extend beyond dynacard (artificial_lift, mooring, installation)
- `orcaflex/` — model generation, post-processing, batch management
- `fatigue/` — S-N curves, rainflow counting, damage accumulation

**Tier 3 — Medium**:
- `signal/` — signal processing, FFT, filtering
- `structural/` — stress analysis, code checks
- `hydrodynamics/` — RAO, wave spectra, diffraction

### Phase 3 — Test Writing Strategy
- **Per module**: Write tests for public API surface first, then edge cases
- **Test fixtures**: Create shared fixture files for common data patterns (YAML configs, sample DataFrames)
- **No mocks**: Use real data per testing rules; mock only external APIs (OrcaFlex license, file I/O)
- **OrcaFlex-dependent tests**: Use `pytest.importorskip('OrcFxAPI')` to skip when unavailable. This is consistent with "mock only external APIs" rule.
- **Naming**: `test_<module>_<function>_<scenario>_<expected>` convention
- **Target**: 20-30 tests per module, each <100ms

### Phase 4 — CI Coverage Reporting
- Add `pytest-cov` configuration with `--cov=digitalmodel --cov-report=html`
- Set minimum coverage threshold per package in CI config
- Generate coverage badge for README

### Phase 5 — Incremental Milestones
1. Fix infrastructure → verify ~488 files discovered (Phase 1: 1 week)
2. Tier 1 modules to 80% — estimate per-module effort after Phase 1 audit reveals actual test gap sizes
3. Tier 2 modules to 80% — schedule after Tier 1 assessment
4. Tier 3 modules to 80% — schedule after Tier 2 assessment
5. Overall 80%+ with CI gate — staged per-package thresholds, not all-at-once
**Note**: Original 7-week timeline is aspirational. Given 1,097 source files and external dependencies, realistic timeline will be established after Phase 1 audit.

### Dependencies
- Requires `assetutilities` on PYTHONPATH (many digitalmodel modules import from it)
- OrcaFlex tests need conditional skip when OrcFxAPI not installed
- Some visualization tests may need matplotlib/plotly — use image comparison or output validation

### Review Notes
- **Verdict**: Claude: MINOR | Codex: MAJOR | Gemini: APPROVE
- **Actions taken**:
  - Updated test file count from 335 to ~488 (verified on disk)
  - Added package installability check to Phase 1 (root cause may be packaging, not just conftest)
  - Added `pytest.importorskip('OrcFxAPI')` strategy for OrcaFlex tests
  - Replaced fixed 7-week timeline with phased milestone approach — estimate after Phase 1 audit
  - Added staged CI coverage gating (per-package thresholds after baseline stabilization)
  - Codex and Claude agree timeline is unrealistic; Codex rated MAJOR for this reason
- *Cross-reviewed: 2026-02-09*

*Approved by user: 2026-02-09*

---
*Source: test coverage audit across workspace-hub repositories*
