source_dir: "client_projects/energy_firm_sd_support/xom/000 ENIGMA/"
scan_date: "2026-02-13"
total_files: 95
scan_summary: "Phase 0-1 audit of ENIGMA safety analysis codebase for overlap with worldenergydata safety modules"

legal_findings:
  client_references_found: 368
  critical_patterns:
    - pattern: "databricks|mlflow|azure"
      count: 295
      severity: "exclude"
      description: "Cloud platform-specific code (Databricks notebooks, MLflow tracking, Azure Spark SQL)"
    - pattern: "nabors"
      count: 71
      severity: "exclude"
      description: "Contractor-specific references (rig names, company identifiers)"
    - pattern: "xom|exxon|enigma"
      count: 2
      severity: "exclude"
      description: "Direct client references"
    - pattern: "spark.sql|sandbox\\."
      estimated_count: "~80%"
      severity: "exclude"
      description: "Databricks Spark SQL queries against client-specific sandbox databases"
  recommendation: "All files require significant refactoring to remove client infrastructure dependencies. Direct porting not viable without complete data layer abstraction."

categories:
  databricks_notebooks:
    count: 76
    description: "Databricks notebook source files with MAGIC commands and Spark SQL"
    files:
      - "cod/incident_analysis.py"
      - "cod/nlp/data_exploration.py"
      - "cod/nlp/d_forms_d180.py"
      - "cod/nlp/Fit_tfidf.py"
      - "cod/nlp/sshe_NLP/Analysis/*.py (15 files)"
      - "cod/nlp/sshe_NLP/Experiments/*/*.py (26 files)"
      - "cod/nlp/sshe_NLP/PreProcessing/*.py (17 files)"
      - "cod/nlp/hypothesis_testing/*.py (14 files)"
    exclusion_reason: "Tightly coupled to Databricks platform - spark.sql(), MAGIC commands, sandbox database schema"

  nlp_ml_experiments:
    count: 26
    description: "NLP/ML model experiments across different architectures"
    subdirectories:
      BERT_models: 2
      doc2vec_models: 2
      LSTM_models: 3
      tfidf_models: 9
      time_series_classification: 1
      word2vec_models: 2
    files:
      - "cod/nlp/sshe_NLP/Experiments/BERT_models/BERT_Tuner_Obs_Cards.py"
      - "cod/nlp/sshe_NLP/Experiments/BERT_models/iscout_Hazard_BERT_Random_Forest.py"
      - "cod/nlp/sshe_NLP/Experiments/tfidf_models/Observations_tfidf_Random_Forest.py"
      - "cod/nlp/sshe_NLP/Experiments/tfidf_models/Obs_Causes_tfidf_Random_Forest.py"
      - "cod/nlp/sshe_NLP/Experiments/LSTM_models/Obs_Cards_LSTM_cleaned_text.py"
      - "cod/nlp/sshe_NLP/Experiments/time_series_classification/Predict_incident_window_end_RF_inc_window.py"
    exclusion_reason: "Experiment scripts with hardcoded client data paths and Databricks-specific imports"

  preprocessing_pipelines:
    count: 17
    description: "Data preprocessing and feature engineering for observation cards and incidents"
    files:
      - "cod/nlp/sshe_NLP/PreProcessing/preprocess_obs_cards_cloned.py"
      - "cod/nlp/sshe_NLP/PreProcessing/preprocess_incidents_cloned.py"
      - "cod/nlp/sshe_NLP/PreProcessing/Cards_Ratings_Stats.py"
      - "cod/nlp/sshe_NLP/PreProcessing/Fit_tfidf.py"
      - "cod/nlp/sshe_NLP/PreProcessing/Filter_Bad_Cards.py"
      - "cod/nlp/sshe_NLP/PreProcessing/PreProcess_Obs_Causes.py"
      - "cod/nlp/sshe_NLP/PreProcessing/PreProcess_Obs_Types.py"
    exclusion_reason: "Contractor-specific data schemas (HP, Nabors) with hardcoded rig ID parsing logic"

  analysis_tools:
    count: 15
    description: "Analysis scripts for incidents, observations, and correlations"
    files:
      - "cod/nlp/sshe_NLP/Analysis/Observations_Incidents_Correlations.py"
      - "cod/nlp/sshe_NLP/Analysis/BERT_embeddings.py"
      - "cod/nlp/sshe_NLP/Analysis/Incidents_stats.py"
      - "cod/nlp/sshe_NLP/Analysis/Classify_Obs_Causes.py"
      - "cod/nlp/sshe_NLP/Analysis/Classify_Obs_Types.py"
    exclusion_reason: "Databricks notebook format with client-specific data sources"

  hypothesis_testing:
    count: 14
    description: "Hypothesis testing scripts for observation-incident correlations"
    files:
      - "cod/nlp/hypothesis_testing/sshe_lib_preprocess_obs_cards.py"
      - "cod/nlp/hypothesis_testing/sshe_lib_preprocess_incidents.py"
      - "cod/nlp/hypothesis_testing/sshe_lib_time_series.py"
      - "cod/nlp/hypothesis_testing/sshe_lib_correlation_tools.py"
      - "cod/nlp/hypothesis_testing/sshe_lib_plot.py"
      - "cod/nlp/hypothesis_testing/sshe_main_observation_gradient.py"
    exclusion_reason: "Contractor-specific preprocessing with Databricks Spark SQL dependencies"

  library_utilities:
    count: 5
    description: "Utility libraries for feature engineering, correlation, and TF-IDF"
    files:
      - "cod/nlp/sshe_NLP/Lib/sshe_lib_feature_engineering.py"
      - "cod/nlp/sshe_NLP/Lib/sshe_lib_correlation_tools.py"
      - "cod/nlp/sshe_NLP/Lib/tfidf_utils.py"
      - "cod/nlp/sshe_NLP/Lib/installs.py"
      - "cod/nlp/sshe_NLP/Lib/installs_light.py"
    exclusion_reason: "Mixed - installs are Databricks-specific, but other utilities could be refactored"

  models:
    count: 1
    description: "Model architecture definitions"
    files:
      - "cod/nlp/sshe_NLP/Models/BERT_models.py"
    exclusion_reason: "Databricks notebook with client-specific paths"

  config_analysis:
    count: 5
    description: "Configuration and standalone analysis scripts"
    files:
      - "cod/she_config.py"
      - "cod/incident_analysis_2022.py"
      - "cod/all_followups_analysis.py"
      - "cod/maintenance_analysis.py"
      - "cod/npt_analysis.py"
      - "cod/ptw_analysis.py"
    exclusion_reason: "Hardcoded file paths to client project directories (D:\\GitHub\\client_projects\\...)"

overlap_analysis:
  duplicate:
    description: "Functionality that worldenergydata already has implemented"
    mappings:
      - enigma_file: "cod/nlp/sshe_NLP/Lib/sshe_lib_feature_engineering.py"
        worldenergydata_equivalent: "src/worldenergydata/safety_analysis/analysis/feature_engineering.py"
        overlap_percentage: 85
        notes: "Both implement windowed time series feature extraction (mean, std, skewness, kurtosis, FFT). ENIGMA version has Databricks comments; worldenergydata version is generalized with asset_id."

      - enigma_file: "cod/nlp/sshe_NLP/Lib/sshe_lib_correlation_tools.py"
        worldenergydata_equivalent: "src/worldenergydata/safety_analysis/analysis/correlation.py"
        overlap_percentage: 90
        notes: "Both compute lagged cross-correlation between observations and incidents. ENIGMA uses rig_id; worldenergydata uses asset_id."

      - enigma_file: "cod/nlp/hypothesis_testing/sshe_lib_time_series.py"
        worldenergydata_equivalent: "src/worldenergydata/safety_analysis/analysis/time_series.py"
        overlap_percentage: 80
        notes: "Both build time series from irregular events, approximate asset activity periods, create binned counts. ENIGMA has Databricks format; worldenergydata is generalized."

      - enigma_file: "cod/nlp/sshe_NLP/Lib/tfidf_utils.py"
        worldenergydata_equivalent: "src/worldenergydata/safety_analysis/nlp/tfidf_vectorizer.py"
        overlap_percentage: 75
        notes: "Both implement TF-IDF vectorization for safety text. Worldenergydata version is config-driven."

      - enigma_file: "cod/nlp/sshe_NLP/Models/BERT_models.py"
        worldenergydata_equivalent: "src/worldenergydata/safety_analysis/nlp/bert_pipeline.py"
        overlap_percentage: 70
        notes: "Both use HuggingFace transformers for BERT embeddings + sklearn classifiers. Worldenergydata has model registry and Pydantic configs."

      - enigma_file: "cod/nlp/sshe_NLP/PreProcessing/preprocess_obs_cards_cloned.py"
        worldenergydata_equivalent: "src/worldenergydata/safety_analysis/nlp/text_preprocessing.py"
        overlap_percentage: 60
        notes: "Both preprocess safety observation text. ENIGMA is contractor-specific (HP/Nabors); worldenergydata is generic with YAML stop words."

      - enigma_file: "cod/nlp/sshe_NLP/Analysis/Observations_Incidents_Correlations.py"
        worldenergydata_equivalent: "src/worldenergydata/safety_analysis/analysis/correlation.py + reports/correlation_report.py"
        overlap_percentage: 65
        notes: "ENIGMA combines analysis + visualization in Databricks notebook; worldenergydata separates analysis from reporting."

  extend:
    description: "ENIGMA functionality that could extend existing worldenergydata modules"
    mappings:
      - enigma_file: "cod/nlp/sshe_NLP/Analysis/Classify_Obs_Causes.py"
        base_module: "src/worldenergydata/safety_analysis/nlp/classification_pipeline.py"
        extension: "Observation cause classification (root cause taxonomy)"
        refactoring_needed: "Remove Databricks Spark SQL, generalize to any data source, add to taxonomy module"

      - enigma_file: "cod/nlp/sshe_NLP/Analysis/Classify_Obs_Types.py"
        base_module: "src/worldenergydata/safety_analysis/nlp/classification_pipeline.py"
        extension: "Observation type classification (green/red, safe/unsafe)"
        refactoring_needed: "Remove contractor-specific logic, integrate with existing taxonomy classifiers"

      - enigma_file: "cod/nlp/sshe_NLP/Analysis/SME_obs_cards_crosscheck.py"
        base_module: "src/worldenergydata/safety_analysis/reports/"
        extension: "SME rating validation and inter-rater agreement analysis"
        refactoring_needed: "Abstract from Databricks, create generic validation report class"

      - enigma_file: "cod/nlp/sshe_NLP/PreProcessing/Cards_Ratings_Stats_w_Voting.py"
        base_module: "src/worldenergydata/safety_analysis/analysis/incident_aggregation.py"
        extension: "Voting-based consensus rating for observation severity"
        refactoring_needed: "Extract voting algorithm, remove Databricks dependencies"

      - enigma_file: "cod/nlp/sshe_NLP/Experiments/time_series_classification/Predict_incident_window_end_RF_inc_window.py"
        base_module: "src/worldenergydata/safety_analysis/analysis/feature_engineering.py"
        extension: "Time window-based incident prediction using Random Forest"
        refactoring_needed: "Separate experiment logic from data loading, create reusable predictor class"

      - enigma_file: "cod/nlp/hypothesis_testing/sshe_main_observation_gradient.py"
        base_module: "src/worldenergydata/safety_analysis/analysis/time_series.py"
        extension: "Gradient analysis with moving averages for observation trends"
        refactoring_needed: "Add gradient methods to TimeSeriesBuilder class"

      - enigma_file: "cod/nlp/hypothesis_testing/sshe_lib_wellview.py"
        base_module: "src/worldenergydata/safety_analysis/adapters/"
        extension: "WellView data adapter for POB (personnel on board) normalization"
        refactoring_needed: "Create generic POB adapter, remove client-specific fuzzy matching logic"

  unique:
    description: "ENIGMA functionality not present in worldenergydata"
    items:
      - file: "cod/nlp/sshe_NLP/PreProcessing/main_obs_clustering.py"
        functionality: "Unsupervised clustering of observation cards to discover patterns"
        portability: "Medium - needs generic clustering module, remove Databricks dependencies"
        value: "High - could reveal hidden incident precursors"

      - file: "cod/nlp/sshe_NLP/Analysis/Obs_Topics.py"
        functionality: "Topic modeling (LDA/NMF) on observation descriptions"
        portability: "Medium - worldenergydata has no topic modeling module"
        value: "High - useful for discovering emerging safety themes"

      - file: "cod/nlp/sshe_NLP/Analysis/Incs_Topics.py"
        functionality: "Topic modeling on incident descriptions"
        portability: "Medium - same as Obs_Topics"
        value: "High - complements observation topic analysis"

      - file: "cod/nlp/sshe_NLP/PreProcessing/Collect_training_data.py"
        functionality: "Training data collection and labeling workflow"
        portability: "Low - heavily Databricks-dependent"
        value: "Medium - workflow concept useful, implementation not portable"

      - file: "cod/nlp/sshe_NLP/PreProcessing/Filter_Bad_Cards.py"
        functionality: "Data quality filtering for low-quality observation cards"
        portability: "High - logic is generic (length checks, keyword filters)"
        value: "Medium - useful for data quality module"

      - file: "cod/nlp/hypothesis_testing/sshe_lib_pob.py"
        functionality: "Personnel-on-board (POB) aggregation from daily reports"
        portability: "Medium - needs generic POB data source abstraction"
        value: "High - POB normalization critical for rate-based safety metrics"

      - file: "cod/nlp/hypothesis_testing/sshe_rig_info.py"
        functionality: "Environmental context extraction (weather, temperature, wind speed)"
        portability: "Medium - needs generic metadata extraction"
        value: "Medium - environmental factors could correlate with incidents"

      - file: "cod/nlp/d_forms_d180.py"
        functionality: "Parsing D-180 well control readiness checklists"
        portability: "Low - specific to client form structure"
        value: "Low - form-specific, not generalizable"

      - file: "cod/all_followups_analysis.py"
        functionality: "Follow-up action tracking and overdue analysis"
        portability: "Medium - concept is generic, implementation is client-specific"
        value: "Medium - could extend incident_aggregation module"

      - file: "cod/maintenance_analysis.py"
        functionality: "Maintenance work order correlation with safety events"
        portability: "Medium - needs generic work order adapter"
        value: "High - maintenance-safety correlation is valuable insight"

      - file: "cod/npt_analysis.py"
        functionality: "Non-productive time (NPT) analysis"
        portability: "Unknown - file not sampled in detail"
        value: "Medium - operational efficiency vs safety correlation"

      - file: "cod/ptw_analysis.py"
        functionality: "Permit-to-work analysis"
        portability: "Unknown - file not sampled in detail"
        value: "Medium - permit compliance correlation with incidents"

  exclude:
    description: "Files that cannot be ported due to client-specific dependencies"
    items:
      - file: "cod/she_config.py"
        reason: "Hardcoded client project file paths (D:\\GitHub\\client_projects\\...)"

      - file: "cod/nlp/sshe_NLP/Lib/installs.py"
        reason: "Databricks-specific package installation script"

      - file: "cod/nlp/sshe_NLP/Lib/installs_light.py"
        reason: "Databricks-specific package installation script"

      - all_files_with: "# Databricks notebook source"
        count: 76
        reason: "Databricks notebook format with MAGIC commands, Spark SQL against sandbox schema"

      - all_files_with: "spark.sql(\"SELECT * FROM sandbox."
        estimated_count: 60
        reason: "Direct queries to client-specific Databricks sandbox database"

      - all_files_with: "pdf_cards_hp|pdf_cards_nbr|H&P Drilling|Nabors Drilling"
        estimated_count: 25
        reason: "Contractor-specific data schemas and rig naming conventions"

worldenergydata_coverage:
  existing_modules:
    safety_analysis:
      files: 46
      submodules:
        - "adapters/ (4 files) - CSV, HSE database adapters"
        - "analysis/ (6 files) - correlation, decomposition, feature_engineering, incident_aggregation, statistical_tests, time_series"
        - "core/ (2 files) - models.py (Pydantic), schemas.py"
        - "data/ (2 files) - loaders.py, processors.py"
        - "nlp/ (5 files) - bert_pipeline, classification_pipeline, model_registry, text_preprocessing, tfidf_vectorizer"
        - "reports/ (4 files) - base_report, classification_report, correlation_report, incident_report"
        - "risk_index/ (7 files) - CLI, dashboard, data_assembler, methodology, models, normalizer, scorer"
        - "taxonomy/ (3 files) - activity_taxonomy, incident_classifier, CLI"
      strengths: "Generic, config-driven, Pydantic models, separation of concerns, testing coverage"
      gaps: "No topic modeling, no clustering, no POB normalization, no environmental context, no SME voting"

    hse:
      files: 18
      submodules:
        - "acquirers/ (4 files) - BSEE, EPA TRI, OSHA data acquisition"
        - "database/ (2 files) - SQLAlchemy models for HSE incidents"
        - "importers/ (12 files) - BSEE incidents/penalties/statistics, EPA TRI, OSHA, data quality validation"
      strengths: "Public data sources, database integration, data quality validation"
      gaps: "No proprietary contractor data adapters (by design - legal compliance)"

    marine_safety:
      files: 76
      submodules:
        - "acquirers/ (2 files) - NTSB CAROL, USCG MISLE"
        - "analysis/ (14 files) - cause analysis, correlations, incident detection"
        - "database/ (3 files) - DB manager, models, init"
        - "importers/ (13 files) - ATSB, EMSA, IMO, MAIB, MISLE, NTSB, TSB, boating, data validation"
        - "scrapers/ (10 files) - ATSB, NTSB, USCG web scraping"
        - "processors/ (3 files) - base processor, data cleaner, normalizer"
        - "reports/ (1 file) - quality dashboard"
        - "CLI modules (8 files) - correlate, db, export, import, refresh, scrape, stats, utils"
      strengths: "Multi-source data integration, web scraping, LLM classification, correlation matching"
      gaps: "Marine-specific, not drilling/land-based rig focus"

gaps_to_address:
  high_priority:
    - gap: "Topic modeling for observation and incident descriptions"
      enigma_files: ["Obs_Topics.py", "Incs_Topics.py"]
      implementation: "New module: safety_analysis/nlp/topic_modeling.py with LDA/NMF"

    - gap: "Unsupervised clustering for pattern discovery"
      enigma_files: ["main_obs_clustering.py"]
      implementation: "New module: safety_analysis/analysis/clustering.py"

    - gap: "Personnel-on-board (POB) normalization"
      enigma_files: ["sshe_lib_pob.py", "sshe_lib_wellview.py"]
      implementation: "Extend adapters/ with generic POB data source, add normalization to feature_engineering.py"

    - gap: "SME rating validation and voting consensus"
      enigma_files: ["SME_obs_cards_crosscheck.py", "Cards_Ratings_Stats_w_Voting.py"]
      implementation: "New module: safety_analysis/analysis/rating_validation.py"

    - gap: "Maintenance work order correlation"
      enigma_files: ["maintenance_analysis.py"]
      implementation: "Extend incident_aggregation.py with maintenance correlation methods"

  medium_priority:
    - gap: "Environmental context extraction (weather, wind, temperature)"
      enigma_files: ["sshe_rig_info.py"]
      implementation: "New module: safety_analysis/adapters/environmental_adapter.py"

    - gap: "Data quality filtering for observation cards"
      enigma_files: ["Filter_Bad_Cards.py"]
      implementation: "Extend data/processors.py with quality filters"

    - gap: "Gradient and moving average trend analysis"
      enigma_files: ["sshe_main_observation_gradient.py"]
      implementation: "Extend analysis/time_series.py with gradient methods"

  low_priority:
    - gap: "Follow-up action tracking"
      enigma_files: ["all_followups_analysis.py"]
      implementation: "Extend incident_aggregation.py or create new action_tracking.py"

    - gap: "Non-productive time (NPT) analysis"
      enigma_files: ["npt_analysis.py"]
      implementation: "New module if operational data sources become available"

    - gap: "Permit-to-work analysis"
      enigma_files: ["ptw_analysis.py"]
      implementation: "New module if permit data sources become available"

recommendations:
  phase_2_planning:
    - "Focus on unique/extend categories - these add the most value"
    - "Duplicate category files should NOT be ported - worldenergydata already has equivalent or better implementations"
    - "Exclude category files cannot be ported without complete data layer refactoring"
    - "Priority order: topic modeling > clustering > POB normalization > SME validation > maintenance correlation"

  refactoring_strategy:
    - "Extract algorithmic logic from Databricks notebooks into pure Python functions"
    - "Replace spark.sql() with pandas DataFrame inputs (mock data for tests)"
    - "Generalize contractor-specific rig ID parsing to generic asset ID handling"
    - "Create adapter pattern for proprietary data sources (if client provides sanitized exports)"
    - "Add Pydantic config models for all new modules (consistent with worldenergydata style)"
    - "Write pytest tests before implementing (TDD mandate)"

  legal_compliance:
    - "NEVER reference client project paths, rig names, contractor names in ported code"
    - "Replace all D:\\GitHub\\client_projects\\... paths with config-driven data sources"
    - "Remove or genericize all Databricks/Azure/Spark references"
    - "Run legal scan on all ported code before committing"
    - "Document sanitization steps in WRK-072 work queue item"

estimated_porting_effort:
  duplicate_category: "0 hours - do not port, already implemented"
  extend_category: "120-160 hours - significant refactoring needed"
  unique_category: "80-120 hours - new modules with moderate complexity"
  exclude_category: "N/A - cannot port without client data access"
  total_viable_porting: "200-280 hours (5-7 weeks for 1 developer)"

next_steps:
  phase_2:
    - "Prioritize unique/extend items by business value"
    - "Create detailed implementation plans for top 5 items"
    - "Set up test fixtures with synthetic safety data (no client data)"
    - "Implement topic modeling module first (highest value, moderate complexity)"
  phase_3:
    - "Port clustering module with generic sklearn integration"
    - "Add POB normalization to feature engineering"
    - "Implement SME rating validation"
  phase_4:
    - "Extend with maintenance correlation, environmental context"
    - "Add data quality filters and gradient analysis"
