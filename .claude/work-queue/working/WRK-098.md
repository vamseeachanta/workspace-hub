---
id: WRK-098
title: "Clean up 7.1GB large data committed to worldenergydata git history"
status: pending
priority: high
complexity: high
route: C
created_at: 2026-02-08T15:00:00Z
target_repos:
  - worldenergydata
commit:
spec_ref:
related: [WRK-067, WRK-068, WRK-097]
blocked_by: []
synced_to: []
tags: [data-governance, git-lfs, cleanup, large-files]
review:
  cross_review: pending
  iterations: 0
---

# Clean Up Large Data in worldenergydata Git History

## What
Remove ~7.1GB of raw data files from worldenergydata's git history and establish proper .gitignore + pipeline regeneration patterns per DATA_RESIDENCE_POLICY.md.

## Why
OSHA data (6.6GB of CSVs and ZIPs from WRK-067) and BSEE data (526MB from WRK-068) are committed directly to regular git. This bloats the repo, makes clones extremely slow, and violates the new data commit strategy.

## Current State
- `data/modules/hse/raw/osha/`: 50+ CSV files > 10MB each, 13 ZIP files (6.6GB total)
- `data/modules/bsee/`: bin/ and zip/ directories (526MB total)
- `data/modules/hse/raw/epa_tri/`: additional large CSVs
- Git LFS partially configured (BSEE .bin files only), not applied to OSHA CSVs
- Data pipelines exist: `osha_acquirer.py`, `bsee_acquirer.py` can regenerate raw data
- `.gitignore` has a merge conflict and missing rules for large data

## Route C Pipeline
```
Plan (cleanup strategy) → Explore (audit) → Implement (BFG/filter-branch) → Test → Review → Archive
```

## Acceptance Criteria
- [ ] .gitignore merge conflict resolved
- [ ] .gitignore rules added for all raw data > 10MB
- [ ] README.md with regeneration instructions in each .gitignored data directory
- [ ] Git history cleaned (BFG Repo-Cleaner or git-filter-repo) to remove large blobs
- [ ] .gitattributes updated for any 10-100MB files that should use LFS
- [ ] Pre-commit hook or CI check blocks files > 50MB without LFS
- [ ] All data acquisition pipelines verified working (can regenerate .gitignored data)
- [ ] Repo clone size reduced from ~7GB to <500MB
