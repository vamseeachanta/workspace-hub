{"ts":"2026-02-04T04:45:41.024Z","role":"ai","text":"\n\nI'll start by reading all the referenced files to understand the full scope of the WRK-032 implementation.","session":"ef48966e-5b14-4d9f-ae4f-6d8f30d68a2e"}
{"ts":"2026-02-04T04:45:41.488Z","role":"ai","text":"[tool:Read]","session":"ef48966e-5b14-4d9f-ae4f-6d8f30d68a2e"}
{"ts":"2026-02-04T04:45:41.948Z","role":"ai","text":"[tool:Read]","session":"ef48966e-5b14-4d9f-ae4f-6d8f30d68a2e"}
{"ts":"2026-02-04T04:45:46.541Z","role":"ai","text":"[tool:Read]","session":"ef48966e-5b14-4d9f-ae4f-6d8f30d68a2e"}
{"ts":"2026-02-04T04:45:46.791Z","role":"ai","text":"[tool:Read]","session":"ef48966e-5b14-4d9f-ae4f-6d8f30d68a2e"}
{"ts":"2026-02-04T04:45:47.020Z","role":"ai","text":"[tool:Read]","session":"ef48966e-5b14-4d9f-ae4f-6d8f30d68a2e"}
{"ts":"2026-02-04T04:45:47.227Z","role":"ai","text":"[tool:Read]","session":"ef48966e-5b14-4d9f-ae4f-6d8f30d68a2e"}
{"ts":"2026-02-04T04:45:46.486Z","role":"human","text":"[result]","session":"ef48966e-5b14-4d9f-ae4f-6d8f30d68a2e"}
{"ts":"2026-02-04T04:45:51.527Z","role":"human","text":"[result]","session":"ef48966e-5b14-4d9f-ae4f-6d8f30d68a2e"}
{"ts":"2026-02-04T04:45:59.138Z","role":"human","text":"[result]","session":"ef48966e-5b14-4d9f-ae4f-6d8f30d68a2e"}
{"ts":"2026-02-04T04:45:55.168Z","role":"human","text":"[result]","session":"ef48966e-5b14-4d9f-ae4f-6d8f30d68a2e"}
{"ts":"2026-02-04T04:46:03.227Z","role":"human","text":"[result]","session":"ef48966e-5b14-4d9f-ae4f-6d8f30d68a2e"}
{"ts":"2026-02-04T04:46:07.603Z","role":"human","text":"[result]","session":"ef48966e-5b14-4d9f-ae4f-6d8f30d68a2e"}
{"ts":"2026-02-04T04:46:13.006Z","role":"ai","text":"Now let me read the remaining implementation and test files.","session":"ef48966e-5b14-4d9f-ae4f-6d8f30d68a2e"}
{"ts":"2026-02-04T04:46:14.031Z","role":"ai","text":"[tool:Read]","session":"ef48966e-5b14-4d9f-ae4f-6d8f30d68a2e"}
{"ts":"2026-02-04T04:46:14.518Z","role":"ai","text":"[tool:Read]","session":"ef48966e-5b14-4d9f-ae4f-6d8f30d68a2e"}
{"ts":"2026-02-04T04:46:18.855Z","role":"ai","text":"[tool:Read]","session":"ef48966e-5b14-4d9f-ae4f-6d8f30d68a2e"}
{"ts":"2026-02-04T04:46:19.103Z","role":"ai","text":"[tool:Read]","session":"ef48966e-5b14-4d9f-ae4f-6d8f30d68a2e"}
{"ts":"2026-02-04T04:46:19.348Z","role":"ai","text":"[tool:Read]","session":"ef48966e-5b14-4d9f-ae4f-6d8f30d68a2e"}
{"ts":"2026-02-04T04:46:19.564Z","role":"ai","text":"[tool:Read]","session":"ef48966e-5b14-4d9f-ae4f-6d8f30d68a2e"}
{"ts":"2026-02-04T04:46:18.831Z","role":"human","text":"[result]","session":"ef48966e-5b14-4d9f-ae4f-6d8f30d68a2e"}
{"ts":"2026-02-04T04:46:27.913Z","role":"human","text":"[result]","session":"ef48966e-5b14-4d9f-ae4f-6d8f30d68a2e"}
{"ts":"2026-02-04T04:46:23.813Z","role":"human","text":"[result]","session":"ef48966e-5b14-4d9f-ae4f-6d8f30d68a2e"}
{"ts":"2026-02-04T04:46:32.303Z","role":"human","text":"[result]","session":"ef48966e-5b14-4d9f-ae4f-6d8f30d68a2e"}
{"ts":"2026-02-04T04:46:36.039Z","role":"human","text":"[result]","session":"ef48966e-5b14-4d9f-ae4f-6d8f30d68a2e"}
{"ts":"2026-02-04T04:46:40.100Z","role":"human","text":"[result]","session":"ef48966e-5b14-4d9f-ae4f-6d8f30d68a2e"}
{"ts":"2026-02-04T04:46:45.679Z","role":"ai","text":"Now let me read the remaining files -- the 24in spec conversion files.","session":"ef48966e-5b14-4d9f-ae4f-6d8f30d68a2e"}
{"ts":"2026-02-04T04:46:46.687Z","role":"ai","text":"[tool:Read]","session":"ef48966e-5b14-4d9f-ae4f-6d8f30d68a2e"}
{"ts":"2026-02-04T04:46:51.636Z","role":"ai","text":"[tool:Read]","session":"ef48966e-5b14-4d9f-ae4f-6d8f30d68a2e"}
{"ts":"2026-02-04T04:46:51.596Z","role":"human","text":"[result]","session":"ef48966e-5b14-4d9f-ae4f-6d8f30d68a2e"}
{"ts":"2026-02-04T04:46:55.701Z","role":"human","text":"[result]","session":"ef48966e-5b14-4d9f-ae4f-6d8f30d68a2e"}
{"ts":"2026-02-04T04:49:26.072Z","role":"ai","text":"Now I have all the information needed to compile the lessons-learned report. Here is the structured report:\n\n---\n\n# WRK-032 Lessons Learned Report\n\n## Modular OrcaFlex Pipeline Installation Generator with Parametric Campaign Support\n\n**Work Item**: WRK-032 (supersedes WRK-033)\n**Scope**: 10 tasks (T00-T09), 2 new source files, 6 modified source files, 7 new test modules, 209 tests passing\n**Date**: 2026-02-03\n**Agent**: claude-opus-4.5\n\n---\n\n## 1. Cross-Review Process\n\n### What happened\n\nThe plan underwent 3 mandatory iterations of dual review by OpenAI Codex and Google Gemini, as required by the workspace convention. Iteration 1 produced the heaviest feedback (17 suggestions from Codex, 18 from Gemini). Iteration 2 yielded 8 items from Codex (all Low/Very Low, resulting in immediate approval) and 6 from Gemini (2 MEDIUM, 4 LOW). Iteration 3 closed with both reviewers approving, each flagging only a single Low-severity informational note about the `v_angle` type in the roller contract.\n\n### Root cause\n\nThe first iteration's high volume was expected. The plan was drafted as a monolithic document before any external validation. Both reviewers independently surfaced structural problems: Gemini identified the need for `ModularModelGenerator.from_spec()` as a prerequisite (which became T00), while Codex flagged the override mapping and roller geometry formula gaps. The second iteration's MEDIUM items from Gemini were specifically about stale contract documents that had not been updated to reflect plan changes -- a documentation drift issue.\n\n### Resolution\n\nEach iteration's feedback was classified by severity (Critical / Medium / Low / Very Low) and acted on in batch before resubmission. Critical items were never deferred. The stale-contract problem in iteration 2 was resolved by versioning the contract markdown files (Rev 2) and explicitly cross-referencing them with the plan's Phase 3 description. By iteration 3, both reviewers converged on the same single minor observation (roller `v_angle` type), confirming that feedback was genuinely diminishing rather than being artificially suppressed.\n\n### Reusable pattern\n\n- **Three iterations is the right number for this complexity level.** Iteration 1 is structural, iteration 2 catches documentation drift and edge cases, iteration 3 confirms convergence. For simpler plans (fewer than 5 tasks), two iterations would likely suffice. For plans exceeding 15 tasks, consider four.\n- **Classify feedback by severity immediately.** This prevents scope creep from cosmetic suggestions while ensuring critical items are never deferred.\n- **Track which review comment maps to which plan change.** The tasks.md Rev 2 changelog (lines 8-16) explicitly cites the reviewer ID and suggestion number for every change. This audit trail is essential when a third reviewer questions why something was added.\n- **Contracts and plan must be updated atomically.** Gemini's iteration-2 MEDIUM about stale contracts was entirely avoidable if the contract files had been updated in the same commit as the plan revision.\n\n---\n\n## 2. TDD Patterns\n\n### What happened\n\nEvery task (T00 through T09) followed a test-first approach documented in the task spec. The tasks.md file explicitly lists \"Steps (TDD)\" with numbered steps starting from \"Write test\" before any implementation step. Test patterns observed across the 7 test modules include:\n\n- **Arrange-Act-Assert**: Consistently used throughout. Example from `test_sections.py` line 21-25: set up resolver and template (arrange), call `resolver.resolve()` (act), assert result equality (assert).\n- **Parametrized fixtures**: The `validated_spec` fixture (referenced in `test_sections.py` line 85) and `spec_data_24in` / `validated_spec_24in` fixtures in `test_24in_spec.py` provide reusable pre-validated spec objects.\n- **Negative testing with `pytest.raises`**: Used extensively for Pydantic `ValidationError` cases (e.g., `test_24in_spec.py` lines 301-327 test missing required fields and invalid values), and for business logic errors (e.g., `test_legacy_compat.py` lines 55-76 test missing config keys).\n- **One class per behavior group**: Tests are organized as classes named after the behavior under test (e.g., `TestFromLegacyConfigConvertsDeltaElevations`, `TestCampaignGeneratorPreview`). Each class contains one or two focused test methods.\n\n### Root cause\n\nThe TDD discipline was enforced by the workspace rules (`D:\\workspace-hub\\.claude\\rules\\testing.md`) which mandate failing tests before implementation code. The schema-driven nature of this project (Pydantic models with Field constraints) made TDD particularly natural: the Pydantic validation rules are the specification, and tests verify them.\n\n### Resolution\n\nTDD caught at least two design issues early:\n\n1. **The `from_spec()` prerequisite** (T00): When writing tests for T04's `generate_run_specs()`, it became clear that the campaign generator needed to construct `ModularModelGenerator` from in-memory `ProjectInputSpec` objects (not file paths). This need was surfaced during test writing for the campaign flow, not during implementation, which justified adding T00 as a prerequisite discovered in iteration 1.\n\n2. **The `should_generate()` S-lay fix** (T05): The test for generating roller buoys on S-lay models revealed that `BuoysBuilder.should_generate()` was returning `False` for all S-lay models, even those with route rollers. The test exposed the missing `get_effective_rollers()` check at `buoys_builder.py` line 63.\n\n### Reusable pattern\n\n- **For Pydantic schema development, write validation boundary tests first.** Field constraints (`ge=`, `le=`, `min_length=`) define exact boundaries that are trivially testable. Write the boundary tests, then implement the Field definitions to pass them. This was done in T01 for `RollerStation` and T04 for `CampaignMatrix`.\n- **Use `pytest.approx` for all floating-point assertions.** Seen consistently in `test_24in_spec.py` (e.g., line 100: `pytest.approx(8)`) and `test_campaign_integration.py` (lines 397-410). This prevents brittle float comparison failures.\n- **One class per test scenario, not one class per module.** The pattern of `TestFromLegacyConfigConvertsDeltaElevations` (single focused class) is preferable to a monolithic `TestLegacyCompat` class. It makes test discovery and failure diagnosis faster.\n- **Use `tmp_path` for all file I/O tests.** Every test that generates files uses pytest's `tmp_path` fixture, ensuring no test pollution and no cleanup needed. This is especially important for campaign generation tests that create directory trees.\n\n---\n\n## 3. Backward Compatibility\n\n### What happened\n\nThree backward compatibility mechanisms were implemented:\n\n1. **Auto-conversion validator** (`equipment.py` lines 387-400): The `Equipment.resolve_roller_compat` model validator detects when the legacy `rollers` field is set but `roller_arrangement` is not, and automatically constructs a single-station `RollerArrangement` from the legacy data. This runs transparently during Pydantic model construction.\n\n2. **Legacy absorption via `from_legacy_config`** (`campaign.py` lines 136-245): The `CampaignSpec.from_legacy_config(cfg)` classmethod accepts the old `OrcInstallation` configuration dict format (with `delta_elevations` and `reference_depth`) and converts it to the new `CampaignSpec` with `water_depths`.\n\n3. **Deprecation warning** (`OrcInstallation`): Rather than deleting the legacy class, a `DeprecationWarning` was added to its `__init__` method, directing users to `CampaignSpec`. The test at `test_legacy_compat.py` lines 91-103 verifies the warning is emitted and contains the migration target name.\n\n### Root cause\n\nThe project has existing spec files (`30in_pipeline/spec.yml`, `SB-SA/spec.yml`) that use the legacy `rollers` field format. Breaking those files would cascade through the test suite and any downstream consumers. The `from_legacy_config` conversion was needed because `OrcInstallation` has external callers who pass the old dict format.\n\n### Resolution\n\nThe auto-conversion validator pattern worked cleanly because it operates at the Pydantic model layer -- it is invisible to callers and requires no code changes in consuming modules. The `get_effective_rollers()` method (`equipment.py` lines 402-404) provides a single access point that returns the correct `RollerArrangement` regardless of which field was originally populated.\n\nThe fragile aspect was the `from_legacy_config` method, which hardcodes assumptions about the legacy config structure (keys like `\"structure\"`, `\"delta_elevations\"`, `\"BaseFile\"`). If the legacy format varies across projects, this brittle mapping will break. The method was deliberately scoped to `create_model_for_water_depth()` only (as noted in T08: the other two legacy methods handle subsea crane installation, which is out of scope).\n\n### Reusable pattern\n\n- **Model validators for silent migration.** Pydantic `@model_validator(mode=\"after\")` is the ideal place to implement field-level backward compatibility. The old field stays in the schema (accepted but not preferred), and the validator promotes it to the new representation. This is non-breaking and requires zero caller changes.\n- **Always provide a `get_effective_*()` accessor.** When two fields can represent the same concept (legacy vs. new), a single accessor method prevents every consumer from implementing their own fallback logic. `get_effective_rollers()` is a clean example.\n- **Deprecate, do not delete.** Keeping `OrcInstallation` alive with a warning allows a migration period. The test that verifies the warning (`test_legacy_compat.py` line 103: `assert \"CampaignSpec\" in str(caught[0].message)`) ensures the migration guidance is correct.\n- **Scope legacy absorption narrowly.** T08 was deliberately limited to a single method (`create_model_for_water_depth()`). Trying to absorb all three legacy methods in one task would have introduced excessive risk. Constrain the conversion boundary to what you can test.\n\n---\n\n## 4. Monolithic-to-Spec Conversion\n\n### What happened\n\nThe 24-inch pipeline spec was extracted from a monolithic OrcaFlex YAML file (approximately 14,000 lines) and condensed into a 102-line `spec.yml` file at `D:\\workspace-hub\\digitalmodel\\docs\\modules\\orcaflex\\pipeline\\installation\\floating\\24in_pipeline\\spec.yml`. Key decisions during the extraction:\n\n1. **6 FE segments collapsed to 1**: The monolithic file defined 6 separate finite element sections with identical line types. Since they all share the same material and coating properties, the spec uses a single segment with `length: 4900` and `segment_length: 0.5`.\n\n2. **Naming discrepancy documented**: The directory is named `24in_pipeline` but the steel pipe OD is 0.762m, which is 30-inch NPS per API 5L. This discrepancy is documented in three places: the spec.yml header comments (lines 4-6), the test file docstring (`test_24in_spec.py` lines 7-8), and the pipeline dimensions comments (spec.yml line 53).\n\n3. **Omissions noted**: The monolithic file included a winch wire definition (165m, OD=0.08m) connecting a 6D buoy to a fixed point. The modular generator schema does not yet support winch definitions, so this was noted as a future extension in the spec header (lines 8-10) rather than silently dropped.\n\n### Root cause\n\nMonolithic OrcaFlex input files accumulate redundancy over years of manual editing. The 6 identical FE sections existed because the original modeler used them for visual segmentation in OrcaFlex, not because they had different properties. The naming discrepancy (`24in` vs 30-inch NPS) likely originated from a project numbering scheme that was not corrected when the actual pipe size was finalized.\n\n### Resolution\n\nA 28-test validation suite (`test_24in_spec.py`) was written to cross-check every extracted value against the monolithic source. The tests are organized into 8 groups: spec loading (3 tests), metadata (2), environment (6), pipeline (5), equipment (5), simulation (2), generation (5), and negative validation (2). Each numerical value uses `pytest.approx` to handle float representation.\n\n### Reusable pattern\n\n- **Document discrepancies at the point of data, not just in commit messages.** The OD naming mismatch is documented in the spec.yml header, in the test docstring, and inline next to the `outer_diameter` field. Future engineers will encounter the explanation regardless of which file they read first.\n- **Write a comprehensive validation test suite for every spec conversion.** The 28 tests in `test_24in_spec.py` serve as the ground truth. If someone later modifies the spec, a test failure will pinpoint exactly which value drifted.\n- **Note omissions explicitly.** The winch definition was not silently dropped -- it was documented as a known gap (spec.yml lines 8-10). This prevents a future engineer from thinking they extracted the complete model when they did not.\n- **Simplify deliberately and document the simplification.** Collapsing 6 FE segments to 1 reduced complexity by 83% without losing information, because the segments were identical. The spec comment on line 64 reads \"Single segment (all 6 FE sections use same line type)\".\n\n---\n\n## 5. Implementation Orchestration\n\n### What happened\n\nThe 10 tasks followed a dependency graph:\n\n```\nT00 --> T01 --> T02 --> T03 --> T04 (parallel with T05)\n                                 |                    |\n                                 +---------+----------+\n                                           |\n                                          T06 --> T07 --> T08 --> T09\n```\n\nT04 (campaign models) and T05 (BuoysBuilder roller arrangement) were identified as parallelizable in the plan because both depend on T03 but are independent of each other. Each task was delegated to a coder subagent following the orchestrator pattern described in the workspace CLAUDE.md.\n\nThe implementation completed all 10 tasks in a single session with 209 tests passing. New files created: 2 source modules (`schema/campaign.py`, `sections.py`) and 7 test modules. Modified files: 6 (`equipment.py`, `_enums.py`, `__init__.py`, `buoys_builder.py`, `context.py`, `cli.py`).\n\n### Root cause\n\nThe dependency graph was well-defined during the planning phase. Each task's blocked-by chain was explicit in tasks.md (e.g., \"Blocked by: T03\" for T04 and T05). The plan identified the parallel opportunity early, which was confirmed by the reviewers.\n\n### Resolution\n\nThe orchestration followed the workspace convention of using the main Claude Code instance as coordinator, not executor. Each task was executed by a subagent with its own context scope. This prevented context pollution from earlier tasks affecting later ones. The parallel execution of T04 and T05 was possible because:\n\n- T04 introduces new models (`CampaignMatrix`, `CampaignSpec`) that do not touch any existing builder code.\n- T05 modifies `BuoysBuilder` to use `RollerArrangement`, which depends only on T03's schema extension, not on T04's campaign models.\n\nContext management across 10 tasks was handled by keeping the orchestrator's context lean (planning and coordination only) and loading implementation details into subagent contexts as needed. The test suite served as the integration verification at each step -- after each task, the full test suite was run to catch regressions.\n\n### Reusable pattern\n\n- **Identify parallel tasks during planning, not during implementation.** The tasks.md explicitly marks \"Parallelisable: T04 and T05\" in the summary table. This makes the execution plan deterministic.\n- **Run the full test suite after each task, not just the new task's tests.** This is how T05's `should_generate()` regression was caught early. If only T05's new tests had been run, the existing floating spec backward compatibility tests might have broken silently.\n- **Keep the orchestrator stateless between tasks.** The orchestrator should only track which tasks are complete and which are unblocked. Implementation details should live in the subagent's context and in the test suite, not in the orchestrator's memory.\n- **Use the dependency graph to determine review order.** Code review should follow the same topological order as implementation. Reviewing T06 before T04 and T05 makes no sense because T06 depends on both.\n\n---\n\n## 6. Common Pitfalls\n\n### 6.1 Windows pytest invocation\n\n**What happened**: On Windows, `uv run pytest` can fail with import resolution errors for the project package, while `uv run python -m pytest` works reliably.\n\n**Root cause**: The `uv run pytest` invocation may not set `sys.path` correctly for the project's package structure on Windows, particularly when the project uses a `src/` layout. Running via `python -m pytest` ensures the Python interpreter adds the current directory to `sys.path`.\n\n**Resolution**: The project's CLAUDE.md documents `uv run pytest` as the test command, but the CLI docstring in `cli.py` (line 9) uses `uv run python -m digitalmodel.solvers.orcaflex.modular_generator`. For test execution, `uv run python -m pytest` is the reliable invocation on Windows.\n\n**Reusable pattern**: On Windows with `uv` and `src/` layout projects, always use `uv run python -m pytest` rather than `uv run pytest`. Document this in the project's test commands section.\n\n### 6.2 Float formatting in naming templates\n\n**What happened**: The `VariableResolver` needed to handle the case where a water depth of `30.0` (a float) should render as `\"30\"` in directory names, not `\"30.0\"`. Without this, campaign output directories would have names like `pipeline_wd30.0m_calm` instead of `pipeline_wd30m_calm`.\n\n**Root cause**: Python's default `str()` conversion of `30.0` produces `\"30.0\"`. OrcaFlex users expect clean integer-like directory names when the value is a whole number.\n\n**Resolution**: The `VariableResolver.resolve()` method (`sections.py` lines 52-55) includes auto-formatting logic: if a value is a `float` and equals its `int()` conversion, it formats as `str(int(value))`. This handles the common case while still allowing explicit format specifiers (`${water_depth:.0f}`) for non-whole numbers.\n\n**Reusable pattern**: When floats appear in file/directory naming templates, always implement whole-number auto-formatting. Test it explicitly (see `test_sections.py` line 34-39: `test_resolve_whole_number_float_auto_format`).\n\n### 6.3 Missing Pydantic fields in test data\n\n**What happened**: Test helper functions that construct campaign data dicts (e.g., `_make_campaign_data()` in `test_campaign_generator.py`) must include every required field in nested Pydantic models. When a required field is missing, Pydantic raises `ValidationError` with a location chain that can be confusing in test output (e.g., `base -> pipeline -> coatings -> corrosion -> thickness`).\n\n**Root cause**: Pydantic v2's strict validation rejects missing required fields with detailed error chains. The deeply nested schema (`CampaignSpec.base.pipeline.coatings.corrosion.thickness`) means omitting any leaf field causes a failure at the top level that requires tracing multiple levels to diagnose.\n\n**Resolution**: All test helper functions were written with every required field present, including seemingly optional nested structures. The helper `_make_campaign_data()` in `test_campaign_generator.py` (lines 14-95) serves as the canonical minimal valid campaign dict, carefully constructed to satisfy all nested validators.\n\n**Reusable pattern**: For deeply nested Pydantic models, create a single canonical test data factory function per model and reuse it across all test files. Modify copies of the canonical data for negative tests (using `copy.deepcopy` as seen in `test_24in_spec.py` line 309) rather than constructing partial dicts from scratch.\n\n### 6.4 OD naming discrepancies\n\n**What happened**: The directory `24in_pipeline` contains a spec with OD 0.762m, which is 30-inch NPS. This was discovered during data extraction from the monolithic file.\n\n**Root cause**: Legacy project naming often uses approximate or contractual sizes rather than actual engineering dimensions. The \"24 inch\" may refer to a project code, a pipe ID (inner diameter), or a procurement designation that does not match the API 5L NPS.\n\n**Resolution**: The discrepancy was documented in three locations (spec header, test docstring, inline comment) as described in section 4 above. The directory was not renamed because it matches the original project folder structure, and renaming would break existing references.\n\n**Reusable pattern**: When converting monolithic data to structured specs, always cross-check dimension-derived names against actual values. Document discrepancies at the point of data. Do not rename directories silently, as this breaks external references -- document and move on.\n\n### 6.5 Circular import risk with campaign generation\n\n**What happened**: The `CampaignGenerator.generate()` method (`campaign.py` line 447) imports `ModularModelGenerator` inside the method body rather than at module level. This is because `campaign.py` lives inside the schema subpackage, while `ModularModelGenerator` is in the parent `__init__.py`, creating a potential circular import.\n\n**Root cause**: The schema layer (`schema/campaign.py`) needs to invoke the generator layer (`__init__.py`), but the generator layer imports from the schema layer. This bidirectional dependency is an inherent tension in the module structure.\n\n**Resolution**: A local import inside the method body breaks the circular import chain at runtime. This is a pragmatic solution that works but is slightly fragile -- if someone refactors the import structure, they need to understand why the import is local.\n\n**Reusable pattern**: When a schema module needs to invoke a generator/engine module that imports from the same schema, use a local import inside the method body. Add a comment explaining why (as done at `campaign.py` line 447: `# Import here to avoid circular imports`). Consider whether this signals a need to refactor the module boundary.\n\n### 6.6 Pydantic `model_copy(deep=True)` for spec mutation\n\n**What happened**: The `generate_run_specs()` method (`campaign.py` line 290) uses `self.base.model_copy(deep=True)` to create independent copies of the base spec before applying overrides. Without `deep=True`, nested Pydantic models share references, and mutating one run's spec would corrupt the base.\n\n**Root cause**: Pydantic v2's `model_copy()` performs a shallow copy by default. For a deeply nested model like `ProjectInputSpec` (which contains `Environment`, which contains `Current` with a `profile` list), shallow copy leads to shared mutable state.\n\n**Resolution**: Using `deep=True` ensures complete independence. This was identified as a requirement during test writing for T04, where the test verifies that each yielded spec is independent (tasks.md line 147: \"Uses `model_copy(deep=True)` (verify by checking spec is independent)\").\n\n**Reusable pattern**: When generating variants of Pydantic models in a loop, always use `model_copy(deep=True)`. Write a test that mutates one variant and verifies the original is unchanged.\n\n---\n\n## Summary of Top Reusable Patterns\n\n| # | Pattern | First Seen In | Applicable To |\n|---|---------|---------------|---------------|\n| 1 | Three-iteration cross-review for complex plans | Plan review cycle | Any plan with 5+ tasks |\n| 2 | Pydantic model validators for silent backward compat migration | `Equipment.resolve_roller_compat` | Any schema evolution |\n| 3 | Canonical test data factory per Pydantic model | `_make_campaign_data()` | All Pydantic model testing |\n| 4 | Whole-number float auto-formatting in naming templates | `VariableResolver.resolve()` | Any template-based file naming |\n| 5 | Document discrepancies at point of data (3 locations) | 24in spec OD note | Any data extraction/conversion |\n| 6 | Identify parallel tasks during planning, not implementation | T04/T05 parallelization | Multi-task implementation plans |\n| 7 | `model_copy(deep=True)` for Pydantic variant generation | `generate_run_specs()` | Any parametric sweep over Pydantic models |\n| 8 | `uv run python -m pytest` on Windows | Test execution | All Windows uv projects |\n| 9 | `get_effective_*()` accessor for dual-field compat | `get_effective_rollers()` | Any model with legacy + new fields |\n| 10 | Local imports to break circular dependencies (documented) | `CampaignGenerator.generate()` | Schema-to-engine bidirectional deps |","session":"ef48966e-5b14-4d9f-ae4f-6d8f30d68a2e"}
