# ============================================================================
# pytest.ini - Universal Configuration Template for workspace-hub
# ============================================================================
#
# Purpose: Standardized pytest configuration for all 25 Python repositories
# Version: 1.0.0
# Last Updated: 2025-01-13
# Status: Universal template for all workspace-hub repositories
#
# Usage:
#   1. Copy this file to your repository root as pytest.ini
#   2. Uncomment/modify sections based on your repository tier
#   3. Override specific values in tox.ini or CI/CD as needed
#
# Repository Tiers:
#   Tier 1 (Production Critical): digitalmodel, energy, frontierdeepwater
#     - Coverage threshold: 85%
#     - Timeout: 300s (conservative)
#     - Parallel: Limited to 4 workers
#
#   Tier 2 (Development Active): aceengineercode, assetutilities, worldenergydata
#     - Coverage threshold: 80%
#     - Timeout: 300s
#     - Parallel: Up to 8 workers
#
#   Tier 3 (Maintenance): All others
#     - Coverage threshold: 75%
#     - Timeout: 600s (more relaxed)
#     - Parallel: Up to 12 workers
#
# ============================================================================

[pytest]

# ============================================================================
# Core Test Discovery Settings
# ============================================================================
# Defines how pytest discovers and recognizes test files and functions

# Directory containing tests (relative to repository root)
testpaths = tests

# Patterns for test file discovery
# Examples: test_module.py, module_test.py
python_files = test_*.py *_test.py

# Patterns for test class discovery
# Classes must start with "Test" prefix
python_classes = Test*

# Patterns for test function discovery
# Functions must start with "test_" prefix
python_functions = test_*

# Patterns for test method discovery in classes
# Methods must start with "test_" prefix
python_methods = test_*

# ============================================================================
# Test Markers - Categorize tests for selective execution
# ============================================================================
# Use markers with: pytest -m <marker_name>
# Run specific markers: pytest -m "unit or integration"
# Skip markers: pytest -m "not slow"
#
# Guidelines:
#   - Mark every test with at least one category marker (unit/integration/e2e)
#   - Add secondary markers for test characteristics (slow, flaky, etc.)
#   - Use markers to skip problematic tests locally while working
#   - Use markers to control CI/CD test selection

markers =
    # ========================================================================
    # PRIMARY TEST CATEGORIES (Use exactly one per test)
    # ========================================================================

    unit: Unit tests - Test single functions/methods in isolation
           Usage: @pytest.mark.unit
           Characteristics: Fast (<100ms), no I/O, no external dependencies
           Example: def test_calculate_discount():
                        result = calculate_discount(100, 0.1)
                        assert result == 90
           CI/CD: Always run, fail build if any fail
           Local: Run first for quick feedback

    integration: Integration tests - Test interaction between components
                 Usage: @pytest.mark.integration
                 Characteristics: Moderate speed (100ms-2s), real dependencies
                 Example: def test_user_login_workflow():
                              user = create_test_user()
                              assert login(user)
                 CI/CD: Run in full suite, some can timeout
                 Local: Run before commits

    e2e: End-to-end tests - Test complete workflows from user perspective
         Usage: @pytest.mark.e2e
         Characteristics: Slow (2s+), full system, real/simulated users
         Example: def test_complete_purchase_flow():
                      login() → add_to_cart() → checkout()
         CI/CD: Run in staging environment, can skip in PR
         Local: Run before release, skip during development

    # ========================================================================
    # PERFORMANCE CHARACTERISTICS (Add to primary category)
    # ========================================================================

    slow: Tests that take significant time to execute (>2 seconds)
          Usage: @pytest.mark.unit @pytest.mark.slow
          Rationale: Separate slow tests to enable quick feedback
          CI/CD: Can run in parallel, may timeout
          Local: Run separately or with -k "not slow"
          Default Timeout: 300 seconds (see timeout setting below)

    flaky: Tests that sometimes fail intermittently (non-deterministic)
           Usage: @pytest.mark.integration @pytest.mark.flaky
           Rationale: Rerun flaky tests multiple times before failing
           CI/CD: Run with --tb=short for easier debugging
           Action: Fix flaky tests immediately (indicates race conditions)
           How to Fix: Add explicit waits, mock timing, fix async issues

    # ========================================================================
    # DEPENDENCY-BASED MARKERS (Add to primary category)
    # ========================================================================

    database: Tests requiring database access (fixtures handle setup/teardown)
              Usage: @pytest.mark.database @pytest.mark.integration
              Characteristics: Requires DB connection, modifies state
              CI/CD: Run with database fixtures, cleanup after
              Local: Skip if DB not available (pytest -m "not database")
              Cleanup: Database is rolled back after each test

    api: Tests requiring external API calls (mocked or real)
         Usage: @pytest.mark.api @pytest.mark.integration
         Characteristics: Network calls, may be slow, mocked by default
         CI/CD: Mocked in CI/CD, real in staging tests
         Local: Use mocked responses (see pytest-mock)
         Mocking: Use responses library or unittest.mock

    scrapy: Tests for Scrapy spiders or web scraping functionality
            Usage: @pytest.mark.scrapy @pytest.mark.integration
            Characteristics: Network calls, may be rate-limited
            CI/CD: Run with rate limiting, can skip in PR
            Local: Skip by default (pytest -m "not scrapy")
            Fixtures: Use scrapy-splash or headless browser

    selenium: Tests using Selenium for browser automation
              Usage: @pytest.mark.selenium @pytest.mark.e2e
              Characteristics: Very slow, requires browser, full UI testing
              CI/CD: Run in staging, skip in PR
              Local: Skip by default (pytest -m "not selenium")
              Headless: Use headless Chrome/Firefox in CI/CD

    # ========================================================================
    # DOMAIN-SPECIFIC MARKERS (Add to primary category)
    # ========================================================================

    llm: Tests involving LLM calls (Claude, GPT, Gemini)
         Usage: @pytest.mark.llm @pytest.mark.integration
         Characteristics: Slow, expensive, non-deterministic
         CI/CD: Skip by default, run nightly with budget limits
         Local: Skip by default (pytest -m "not llm")
         Cost: Each LLM call costs money, limit in CI/CD
         Mocking: Use response fixtures for fast development

    # ========================================================================
    # MARKER USAGE COMBINATIONS (Examples)
    # ========================================================================

    # Fast unit test:
    #   @pytest.mark.unit
    #   def test_simple_calculation(): ...

    # Slow integration test with database:
    #   @pytest.mark.integration
    #   @pytest.mark.slow
    #   @pytest.mark.database
    #   def test_user_creation_workflow(): ...

    # Flaky test that needs multiple retries:
    #   @pytest.mark.integration
    #   @pytest.mark.flaky
    #   def test_async_message_delivery(): ...

    # Skip problematic test locally:
    #   pytestmark = pytest.mark.skip(reason="Requires external service")

    # Skip only in CI/CD:
    #   @pytest.mark.skipif(os.getenv('CI'), reason="Requires local setup")


# ============================================================================
# Test Execution Options
# ============================================================================

# Show extra test summary info: passed, failed, skipped, xfailed, xpassed, warnings
addopts =
    # ========================================================================
    # Display Options
    # ========================================================================

    # Verbose output - shows each test as it runs
    --verbose

    # Show local variable values in tracebacks
    --showlocals

    # Show summary of test results at end
    --tb=short

    # Display warnings (helps catch deprecation issues)
    -W default

    # ========================================================================
    # Strict Mode (Fail on unexpected behavior)
    # ========================================================================

    # Fail test run if any unknown markers are used (catch typos)
    --strict-markers

    # Fail test run if any undefined fixtures are requested
    --strict-config

    # ========================================================================
    # Coverage Options (Optional - enable with -m)
    # ========================================================================
    # Uncomment coverage options below or run:
    # pytest --cov=src --cov-report=html

    # Coverage from 'src' directory
    --cov=src

    # Report missing lines in terminal
    --cov-report=term-missing:skip-covered

    # Generate HTML coverage report
    --cov-report=html:htmlcov

    # Generate XML for CI/CD integration (Codecov, etc.)
    --cov-report=xml

    # ========================================================================
    # Performance Options
    # ========================================================================

    # Show slowest N tests
    --durations=10

    # Show which fixtures are unused (helps clean up)
    # Uncomment for optimization: --unused-fixtures

    # ========================================================================
    # Parallel Execution (Optional - requires pytest-xdist)
    # Install: pip install pytest-xdist
    # Uncomment below for automatic parallel execution
    # ========================================================================

    # Use 'auto' to match number of CPU cores
    # Tier 1 repos: limit to 4 workers (-n 4)
    # Tier 2 repos: limit to 8 workers (-n 8)
    # Tier 3 repos: limit to 12 workers (-n 12)
    # Uncomment one based on your repository tier:

    # -n auto          # Auto-detect (CAREFUL - may overwhelm CI/CD)
    # -n 4             # Tier 1: Production Critical (digitalmodel, energy, etc.)
    # -n 8             # Tier 2: Development Active (aceengineercode, worldenergydata, etc.)
    # -n 12            # Tier 3: Maintenance (all others)


# ============================================================================
# Coverage Configuration (for --cov options above)
# ============================================================================
# These settings control what gets measured for code coverage

[coverage:run]

# Source directory to measure coverage
source = src

# Files/directories to exclude from coverage
omit =
    */tests/*
    */venv/*
    */__pycache__/*
    */site-packages/*
    setup.py
    conftest.py

# Lines to exclude from coverage (use in code with # pragma: no cover)
exclude_lines =
    # Standard pragma
    pragma: no cover

    # Don't complain about debug code
    def __repr__
    if self\.debug

    # Don't complain about defensive programming
    raise AssertionError
    raise NotImplementedError

    # Don't complain about test code
    if __name__ == .__main__.:

    # Don't complain about abstract methods
    @abstractmethod
    @abc.abstractmethod

    # Don't complain about type hints that aren't evaluated at runtime
    if TYPE_CHECKING:


[coverage:report]

# Decimal places in coverage report
precision = 2

# Show lines that weren't covered
show_missing = True

# Show files with 100% coverage (helpful for motivation!)
skip_covered = False

# Minimum coverage percentage required (TIER-SPECIFIC - see below)
# Tier 1 (Production Critical): 85%
# Tier 2 (Development Active): 80%
# Tier 3 (Maintenance): 75%
#
# Set based on your repository tier:
fail_under = 85

# Only fail if coverage dropped (vs absolute threshold)
# Uncomment to use relative targets instead:
# fail_under = 75


[coverage:html]

# Directory for HTML coverage report
directory = htmlcov


# ============================================================================
# Timeout Configuration
# ============================================================================
# Tests that exceed this timeout are killed (prevents hanging)
# Requires: pip install pytest-timeout

timeout = 300

# Timeout for markers:
# unit: 5 seconds (fast)
# integration: 30 seconds (moderate)
# e2e: 120 seconds (slow)
# slow: 300 seconds (very slow)
#
# Override per-test with: @pytest.mark.timeout(60)


# ============================================================================
# Async Test Configuration
# ============================================================================
# Requires: pip install pytest-asyncio
# Run: pytest tests/

# How asyncio is handled in tests
# Options: "auto", "strict", "legacy"
# auto: Automatically detect and run async tests (RECOMMENDED)
asyncio_mode = auto


# ============================================================================
# Logging Configuration
# ============================================================================
# Control what logs appear during test execution

# Log level for pytest logs
log_level = INFO

# Log CLI output level
log_cli_level = INFO

# Enable/disable log capture
log_capture = true

# Log format
log_format = %(asctime)s [%(levelname)8s] %(message)s
log_date_format = %Y-%m-%d %H:%M:%S


# ============================================================================
# Minimum Python Version
# ============================================================================
# Fail if pytest is run with older Python
minversion = 3.8


# ============================================================================
# Console Output Capture
# ============================================================================

# Capture output mechanism: 'fd', 'sys', or 'no'
# fd: Capture at file descriptor level (most reliable)
console_output_style = progress


# ============================================================================
# CUSTOMIZATION GUIDE FOR SPECIFIC REPOSITORIES
# ============================================================================
#
# To override settings in your specific repository, create a local pytest.ini
# with only the sections you want to change. Example:
#
# [pytest]
# # Override just the coverage threshold
# fail_under = 75
#
# # Override timeout for slow repository
# timeout = 600
#
# # Add custom markers
# markers =
#     my_custom_marker: Repository-specific marker
#
# ============================================================================
# TIER-SPECIFIC OVERRIDES
# ============================================================================
#
# Tier 1 Production (digitalmodel, energy, frontierdeepwater):
#   - fail_under = 85
#   - timeout = 300
#   - addopts = -n 4 (parallel with 4 workers max)
#
# Tier 2 Development (aceengineercode, assetutilities, worldenergydata):
#   - fail_under = 80
#   - timeout = 300
#   - addopts = -n 8 (parallel with 8 workers max)
#
# Tier 3 Maintenance (All others):
#   - fail_under = 75
#   - timeout = 600 (more relaxed)
#   - addopts = -n 12 (parallel with 12 workers max)
#
# Create repository-specific pytest.ini overrides:
#   repo_name/pytest.ini
#
# ============================================================================
# COMMON USAGE PATTERNS
# ============================================================================
#
# Run all tests:
#   pytest
#
# Run only unit tests (fast):
#   pytest -m unit
#
# Run unit and integration tests (skip e2e):
#   pytest -m "unit or integration"
#
# Skip slow tests (for quick development feedback):
#   pytest -m "not slow"
#
# Skip external dependency tests (database, api, selenium):
#   pytest -m "not (database or api or selenium)"
#
# Run only integration tests with database:
#   pytest -m "integration and database"
#
# Run tests in parallel (if xdist installed):
#   pytest -n auto
#
# Run with coverage report:
#   pytest --cov=src --cov-report=html
#
# Run specific test file:
#   pytest tests/unit/test_module.py
#
# Run with verbose output and show slowest tests:
#   pytest -v --durations=10
#
# Run and stop on first failure:
#   pytest -x
#
# Run last N tests that failed:
#   pytest --lf
#
# Run tests that failed in last run, then the rest:
#   pytest --ff
#
# Show all marker definitions:
#   pytest --markers
#
# Show all fixtures available:
#   pytest --fixtures
#
# ============================================================================
# TROUBLESHOOTING
# ============================================================================
#
# Problem: "Unknown marker" error
# Solution: Marker is not defined above. Add it to markers = section.
#
# Problem: Tests hang indefinitely
# Solution: Increase timeout setting or add @pytest.mark.timeout(seconds)
#
# Problem: Coverage threshold not being enforced
# Solution: Ensure --cov and --cov-fail-under are in addopts
#
# Problem: Parallel tests fail when they pass individually
# Solution: Tests have shared state. Use fixtures instead of module-level vars
#
# Problem: Async tests not running
# Solution: Ensure pytest-asyncio is installed and asyncio_mode = auto
#
# Problem: Flaky tests pass sometimes, fail others
# Solution: Add @pytest.mark.flaky and rerun plugin: pip install pytest-rerunfailures
#           Use: pytest --reruns 3 -m flaky
#
# ============================================================================
# RECOMMENDED PLUGINS
# ============================================================================
#
# Install with: pip install <package_name>
#
# pytest-cov
#   - Code coverage reporting
#   - Required for: --cov options
#
# pytest-asyncio
#   - Async test support
#   - Required for: async def test_*() functions
#
# pytest-xdist
#   - Parallel test execution
#   - Required for: -n option
#
# pytest-timeout
#   - Test timeout management
#   - Required for: timeout setting
#
# pytest-mock
#   - Easy mocking with mocker fixture
#   - Use: def test_something(mocker): mocker.patch(...)
#
# pytest-rerunfailures
#   - Rerun flaky tests automatically
#   - Use: pytest --reruns 3 -m flaky
#
# pytest-sugar
#   - Better output formatting (optional)
#   - Makes output more readable
#
# pytest-benchmark
#   - Performance benchmarking
#   - Use: def test_perf(benchmark): benchmark(function)
#
# ============================================================================
# VERSION HISTORY
# ============================================================================
#
# 1.0.0 (2025-01-13)
#   - Initial universal template for all 25 workspace-hub repositories
#   - Comprehensive marker definitions with usage examples
#   - Tier-specific coverage thresholds (85%, 80%, 75%)
#   - Detailed comments and customization guide
#   - Support for unit, integration, e2e, slow, flaky, database, api,
#     scrapy, selenium, llm markers
#   - Integration with coverage, asyncio, timeout, logging
#
# ============================================================================
