version: 1.0.0
name: unified-agent-workflow-contract
description: >-
  Workflow-equivalence contract for Claude Code, Codex CLI, and Gemini CLI.
  Session-started provider is orchestrator; others are subagents.

session:
  orchestrator_rule: session_started_agent_is_orchestrator
  handoff_requires_user_approval: true
  default_handoff_allowed: false
  state_file: .claude/work-queue/session-state.yaml

providers:
  supported: [claude, codex, gemini]
  roles:
    orchestrator:
      can_delegate: true
      can_change_stage: true
      can_request_cross_review: true
    subagent:
      can_delegate: false
      can_change_stage: false
      can_execute_assigned_stage_only: true

workflow:
  source_of_truth:
    work_queue_process: .claude/work-queue/process.md
    work_item_root: .claude/work-queue
  stages:
    - capture
    - triage
    - plan_approval_gate
    - implement_tdd
    - cross_review
    - archive

artifacts:
  required:
    - .claude/work-queue/*/WRK-*.md
    - scripts/review/results/*.md
  optional:
    - specs/modules/*.md

gates:
  implementation_requires:
    plan_approved: true
  route_b_c_cross_review_requires:
    attempted_reviewers: [claude, codex, gemini]
    allow_no_output: true
    claude_model: "model-registry.yaml > work_queue_routing.cross_review.route_b/c"
    # Always use best available Claude model for review — never the default/balanced tier.
    # Today: claude-opus-4-6. Tracks model-registry.yaml > latest_models.claude_primary.
    # Rationale: review must have more capability than authoring session (Sonnet) to
    # catch blind spots. When a newer primary model ships, update model-registry.yaml
    # only — behavior-contract inherits automatically via the registry reference.

provider_assignment:
  description: >-
    Provider assignment is a planning-stage decision, not runtime routing.
    During planning, the orchestrator decides which provider(s) execute each
    task based on known strengths. The assignment is recorded in the WRK
    frontmatter (provider, provider_alt, task_agents) and dispatched at
    execution time. Per-phase assignments in task_agents override the
    item-level provider for individual phases.
  fields:
    provider: "Primary executor (claude | codex | gemini)"
    provider_alt: "Optional second executor for dual-agent comparison"
    task_agents: "Per-phase agent map (phase_N: provider # description)"
  strengths:
    codex: "Focused code tasks, single-file changes, algorithms, testing, refactoring, config"
    gemini: "Research, data analysis, summarization, content writing, documents"
    claude: "Multi-file architecture, orchestration, complex integration, sensitive data"

session_locking:
  description: >-
    WRK items are locked when claimed by a session via wrk_claim().
    Only one session can hold a lock at a time. Stale locks (> TTL) are
    auto-reclaimed. Lock is released after review or on error.
  frontmatter_fields:
    locked_by: "Session ID holding the lock"
    locked_at: "ISO 8601 timestamp of lock acquisition"
  lock_ttl_seconds: 7200
  exit_code_collision: 4
  enforced_by:
    claim: execute.sh (pre-dispatch)
    release: review.sh (post-review)

staleness:
  description: >-
    Items in working/ are checked for age on session init and work list.
    Warning at 7 days, critical + move to pending at 14 days.
  thresholds:
    warning_days: 7
    critical_days: 14
  frontmatter_field: "stale: warning | critical"
  enforced_by: [session.sh init, work.sh list]

codex_fallback:
  description: >-
    When Codex returns NO_OUTPUT (empty response on large diffs), a 2-of-3
    consensus from Claude + Gemini can serve as fallback. Explicit REJECT or
    MAJOR from Codex still hard-blocks (no fallback on explicit failure).
  policy:
    codex_approve_or_minor: PASS
    codex_reject_or_major: FAIL (hard block, no fallback)
    codex_no_output:
      claude_and_gemini_both_approve_or_minor: CONDITIONAL_PASS
      otherwise: FAIL
  frontmatter_tag: "codex_fallback: true"
  enforced_by: cross-review.sh

batch_approval:
  description: >-
    Batch plan approval allows reviewing and approving multiple WRK plans
    in a single pass. Lists items with ## Plan but no plan_approved: true.
  commands:
    list: "work.sh --provider <p> approve-batch"
    approve_all: "work.sh --provider <p> approve-batch --approve-all"
  enforced_by: work.sh

pipeline:
  description: >-
    Multi-session pipeline tracking. Sessions register on init and
    deregister on end. Pipeline balance counts sessions per stage
    to recommend under-served stages for new work.
  state_file: .claude/work-queue/pipeline-state.yaml
  commands:
    dashboard: "work.sh --provider <p> status"
    recommend: "work.sh --provider <p> next"
    session_dashboard: "session.sh status"
    deregister: "session.sh end"
  enforced_by: [session.sh, work.sh, workflow-guards.sh]

model_versioning:
  description: >-
    All AI agents MUST use the latest available model versions.
    model-registry.yaml is the single source of truth for model IDs.
    Never hardcode model IDs in scripts — read from the registry or use
    CLI tool defaults (which auto-resolve to latest). When a provider
    releases a new model, update model-registry.yaml and all consumers
    inherit the change.
  source_of_truth: config/agents/model-registry.yaml
  rules:
    - "Scripts MUST NOT contain hardcoded model ID strings"
    - "Use CLI tool defaults (claude, codex, gemini) which resolve to latest"
    - "When a specific model is needed, read from model-registry.yaml latest_models"
    - "Update model-registry.yaml within 7 days of a new model release"
    - "Legacy scripts (scripts/development/ai-workflow/) are exempt until rewrite"
  enforcement:
    check_script: "grep -rn 'claude-[0-9]\\|gpt-[0-9]\\|gemini-[0-9]' scripts/ --include='*.sh' | grep -v model-registry"
    expected: "No matches outside model-registry.yaml and legacy ai-workflow/"

provider_assessment:
  description: >-
    Unbiased, evidence-based assessment of AI agent providers. Assessment
    uses real task outcomes, not synthetic benchmarks. Every cross-review
    cycle and task execution produces assessment data.
  principles:
    - "Same task, same prompt, same context — vary only the provider"
    - "Blind scoring: evaluate outputs without knowing which provider generated them"
    - "Track per-task-type performance, not aggregate scores"
    - "No provider gets special treatment in prompt framing or context"
    - "Assessment data informs task_agents assignments, not provider preferences"
  dimensions:
    correctness:
      weight: 30
      metric: "Does the output solve the stated problem without introducing bugs?"
      scoring: "0=wrong, 1=partial, 2=correct with issues, 3=fully correct"
    completeness:
      weight: 20
      metric: "Are all requirements addressed? Edge cases handled?"
      scoring: "0=missing major parts, 1=core only, 2=most covered, 3=comprehensive"
    code_quality:
      weight: 15
      metric: "Clean, idiomatic, follows project conventions?"
      scoring: "0=poor, 1=functional but messy, 2=clean, 3=exemplary"
    efficiency:
      weight: 10
      metric: "Token usage, latency, number of iterations to succeed"
      scoring: "0=excessive, 1=above average, 2=reasonable, 3=lean"
    instruction_adherence:
      weight: 15
      metric: "Follows CLAUDE.md rules, TDD, commit conventions, etc.?"
      scoring: "0=ignores, 1=partial, 2=mostly, 3=strict adherence"
    autonomy:
      weight: 10
      metric: "Completes without unnecessary clarification requests?"
      scoring: "0=stalls, 1=many questions, 2=few questions, 3=self-sufficient"
  task_type_categories:
    - algorithm        # Single-file focused code
    - testing          # Writing tests
    - architecture     # Multi-module design
    - research         # Information gathering and summarization
    - content          # Documentation, reports, narratives
    - refactoring      # Code restructuring
    - integration      # Cross-module wiring
    - debugging        # Bug diagnosis and fix
  data_collection:
    source: "cross-review verdicts + task execution outcomes"
    storage: ".claude/state/provider-assessments/"
    format: "YAML per assessment, aggregated quarterly"
    template: |
      assessment_id: "WRK-XXX-phase-N"
      date: "YYYY-MM-DD"
      task_type: "<category>"
      task_description: "<1-line>"
      providers_evaluated:
        <provider>:
          correctness: N
          completeness: N
          code_quality: N
          efficiency: N
          instruction_adherence: N
          autonomy: N
          weighted_score: N.NN
          notes: "<observations>"
      verdict: "<provider with highest weighted score>"
  reporting:
    frequency: "After every 10 assessments or quarterly, whichever is first"
    output: "reports/ai-assessment/provider-comparison.md"
    content:
      - "Per-task-type leaderboard"
      - "Trend over time (improving/declining per provider)"
      - "Recommended task_agents mapping updates"
      - "Cost efficiency (score per dollar)"

normalized_verdicts:
  allowed: [APPROVE, MINOR, MAJOR, NO_OUTPUT, CONDITIONAL_PASS, ERROR]
