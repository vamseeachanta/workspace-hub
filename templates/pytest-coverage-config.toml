# ════════════════════════════════════════════════════════════════════════════════
# PYTEST & COVERAGE CONFIGURATION TEMPLATE
# ════════════════════════════════════════════════════════════════════════════════
#
# COPY-PASTE GUIDE:
# 1. For Tier 1 (Work/Production): Use fail_under = 85
# 2. For Tier 2-3 (Active/Maintenance): Use fail_under = 80
# 3. All other settings apply to all tiers
# 4. Paste entire sections into your pyproject.toml
#
# ════════════════════════════════════════════════════════════════════════════════

# ─────────────────────────────────────────────────────────────────────────────
# PART 1: TEST DEPENDENCIES
# ─────────────────────────────────────────────────────────────────────────────
# Add this to [project.optional-dependencies] section

[project.optional-dependencies]

# Core testing - ALWAYS include
test-core = [
    "pytest>=7.4.0,<9.0.0",          # Main test framework
    "pytest-cov>=4.1.0,<5.0.0",      # Coverage integration
]

# Standard testing - Recommended for most projects
test-standard = [
    "pytest>=7.4.0,<9.0.0",
    "pytest-cov>=4.1.0,<5.0.0",
    "pytest-asyncio>=0.21.0,<0.24.0", # Async/await testing
    "pytest-mock>=3.11.0,<4.0.0",     # Mocking utilities
]

# Advanced testing - For complex projects
test-advanced = [
    "pytest>=7.4.0,<9.0.0",
    "pytest-cov>=4.1.0,<5.0.0",
    "pytest-asyncio>=0.21.0,<0.24.0",
    "pytest-mock>=3.11.0,<4.0.0",
    "pytest-benchmark>=4.0.0,<5.0.0", # Performance testing
    "hypothesis>=6.92.0,<7.0.0",      # Property-based testing
    "pytest-xdist>=3.5.0,<4.0.0",     # Parallel test execution
]

# Development tools - Use with test-advanced
dev = [
    "black>=24.0.0,<25.0.0",          # Code formatting
    "ruff>=0.3.0,<1.0.0",             # Fast linting
    "mypy>=1.8.0,<2.0.0",             # Type checking
    "isort>=5.13.0,<6.0.0",           # Import sorting
    "pre-commit>=3.0.0,<4.0.0",       # Git hooks
]

# ─────────────────────────────────────────────────────────────────────────────
# PART 2: PYTEST CORE CONFIGURATION
# ─────────────────────────────────────────────────────────────────────────────
# Add this to [tool.pytest.ini_options] section

[tool.pytest.ini_options]

# Test discovery paths (standard)
testpaths = [
    "tests",
    "test",
]

# Python file patterns to search for tests
python_files = [
    "test_*.py",
    "*_test.py",
    "tests.py",
]

# Test class patterns
python_classes = [
    "Test*",
    "*Tests",
]

# Test function patterns
python_functions = [
    "test_*",
    "*_test",
]

# Minimum pytest version required
minversion = "7.0"

# Command-line options applied to all runs
addopts = [
    # Reporting
    "--verbose",
    "--strict-markers",
    "--strict-config",
    "--tb=short",

    # Performance
    "--timeout=300",
    "--durations=10",

    # Output formatting
    "--color=yes",
]

# Test markers for categorization
markers = [
    "unit: Unit tests (fast, isolated)",
    "integration: Integration tests (multiple components)",
    "performance: Performance benchmarks",
    "slow: Slow tests (deselect with '-m \"not slow\"')",
    "asyncio: Async/await tests",
    "benchmark: pytest-benchmark performance tests",
    "property: Property-based tests with Hypothesis",
    "external: Tests requiring external services",
    "security: Security-focused tests",
    "regression: Regression tests for bug fixes",
    "flaky: Tests known to be flaky",
]

# Filter warnings
filterwarnings = [
    "error",                              # Treat warnings as errors (strict mode)
    "ignore::UserWarning",                # Allow UserWarnings
    "ignore::DeprecationWarning",         # Allow DeprecationWarnings
    "ignore::PendingDeprecationWarning",  # Allow PendingDeprecationWarnings
]

# ─────────────────────────────────────────────────────────────────────────────
# PART 3: COVERAGE CONFIGURATION
# ─────────────────────────────────────────────────────────────────────────────
# Add these [tool.coverage.*] sections

[tool.coverage.run]

# Source directories to measure
source = [
    "src",
    "app",
]

# Paths to exclude from coverage
omit = [
    "*/tests/*",
    "*/test_*.py",
    "*/__pycache__/*",
    "*/venv/*",
    "*/.venv/*",
    "*/env/*",
    "*/virtualenv/*",
    "setup.py",
    "*/migrations/*",
    "*/conftest.py",
    "*/__init__.py",
]

# Measure branch coverage (more thorough)
branch = true

# Parallel test execution coverage collection
parallel = true

# ─────────────────────────────────────────────────────────────────────────────

[tool.coverage.report]

# Precision for coverage percentages
precision = 2

# Show source lines missing coverage
show_missing = true

# Don't skip files with 100% coverage in report
skip_covered = false

# Don't skip files with no executable code
skip_empty = false

# Lines to exclude from coverage calculation
exclude_lines = [
    # Standard exclusions
    "pragma: no cover",
    "def __repr__",
    "raise AssertionError",
    "raise NotImplementedError",

    # Defensive programming
    "if __name__ == .__main__.:",
    "if TYPE_CHECKING:",
    "if typing.TYPE_CHECKING:",

    # Abstract/interface methods
    "@abstract",
    "@abstractmethod",

    # Debug code
    "if self.debug:",
    "if settings.debug:",

    # Protocol/interface definitions
    "pass",
    "\\.\\.\\.",

    # OS-specific
    "if sys.platform",
    "if platform.system",

    # Version-specific
    "if sys.version_info",
]

# ⚠️  CONFIGURE THIS FOR YOUR TIER:
# Tier 1 (Work/Production): 85
# Tier 2-3 (Active/Maintenance): 80
fail_under = 80  # ← Change to 85 for Tier 1 repos

# ─────────────────────────────────────────────────────────────────────────────

[tool.coverage.html]

# Directory for HTML coverage reports
directory = "htmlcov"

# Include branch coverage in HTML report
show_contexts = true

# ─────────────────────────────────────────────────────────────────────────────

[tool.coverage.xml]

# XML output for CI/CD (Codecov, CircleCI, etc)
output = "coverage.xml"

# ─────────────────────────────────────────────────────────────────────────────

[tool.coverage.json]

# JSON output for programmatic access
output = "coverage.json"

# Include code context in JSON
show_contexts = true

# ─────────────────────────────────────────────────────────────────────────────
# PART 4: ADVANCED TEST CONFIGURATIONS (Optional)
# ─────────────────────────────────────────────────────────────────────────────
# Add these sections IF using pytest-benchmark, hypothesis, or pytest-xdist

# Benchmark performance testing (requires: pytest-benchmark)
[tool.pytest.benchmark]

# Skip benchmarks by default (enable with --benchmark-only)
skip = false

# Disable garbage collection during benchmark
disable_gc = true

# Minimum time per benchmark round (in seconds)
min_time = 0.000005

# Maximum time per benchmark round (in seconds)
max_time = 1.0

# Minimum number of rounds
min_rounds = 5

# Timer function ("perf_counter" for high-res timing)
timer = "perf_counter"

# Calibration precision
calibration_precision = 10

# Warmup before benchmarking
warmup = false

# Number of warmup iterations
warmup_iterations = 100000

# ─────────────────────────────────────────────────────────────────────────────

# Property-based testing with Hypothesis (requires: hypothesis)
[tool.hypothesis]

# Maximum number of examples per test
max_examples = 100

# Deadline for test execution (in milliseconds)
deadline = 800

# Database file for saved examples
database_file = ".hypothesis/examples"

# Verbosity level: "quiet", "normal", "verbose"
verbosity = "normal"

# Whether to print failing examples
print_blob = false

# ─────────────────────────────────────────────────────────────────────────────

# Parallel test execution (requires: pytest-xdist)
[tool.pytest-xdist]

# Auto-parallelization based on CPU count
# Run with: pytest -n auto
# Or fix workers: pytest -n 4
# Documentation: https://pytest-xdist.readthedocs.io/

# Note: Configure via CLI instead of this file for flexibility
# Example: pytest -n auto -m "not integration"

# ─────────────────────────────────────────────────────────────────────────────
# PART 5: TESTING COMMAND ALIASES (Optional)
# ─────────────────────────────────────────────────────────────────────────────
# Add this to [project.scripts] or use directly in CI/CD

# Usage examples:
# Run all tests:
#   pytest
#
# Run unit tests only:
#   pytest -m unit
#
# Run with coverage:
#   pytest --cov
#
# Run with HTML report:
#   pytest --cov --cov-report=html
#
# Run parallel tests (4 workers):
#   pytest -n 4
#
# Run benchmarks:
#   pytest -m benchmark --benchmark-only
#
# Run fast tests (skip slow):
#   pytest -m "not slow"
#
# Run integration tests:
#   pytest -m integration
#
# Generate all reports:
#   pytest --cov --cov-report=term --cov-report=html --cov-report=xml

# ════════════════════════════════════════════════════════════════════════════════
# END TEMPLATE
# ════════════════════════════════════════════════════════════════════════════════
